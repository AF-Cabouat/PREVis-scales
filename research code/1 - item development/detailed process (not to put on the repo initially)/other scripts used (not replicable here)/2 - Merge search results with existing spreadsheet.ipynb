{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RV2ScXw0Nvc5",
    "tags": []
   },
   "source": [
    "# Merging csv from search terms in papers with existing spreadsheet\n",
    "\n",
    "Author: Anne-Flore Cabouat\n",
    "Date: March 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "##loading data\n",
    "\n",
    "#csv file from spreadsheet !important! include index column\n",
    "df = pd.read_csv(\"../results/round 3/round1+2.csv\")\n",
    "df_max_index = 1138 #biggest value in index for that file at the begining of the process\n",
    "\n",
    "#csv file to merge\n",
    "df_add = pd.read_csv(\"../results/round 3/paperFound-MERGED.csv\")\n",
    "\n",
    "#output csv file place and name\n",
    "results_path = \"../results/\"\n",
    "csv_out = \"round1+2+3.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#preparing files\n",
    "df_add[\"index\"] = 0 #add index col to new df\n",
    "df_Cols = df.columns.values.tolist()\n",
    "df_add_Cols = df_add.columns.values.tolist()\n",
    "print(f\"Existing columns = {df_Cols}\\n\")\n",
    "print(f\"New file columns = {df_add_Cols}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_Col_keep = [value for value in df_add_Cols if value in df_Cols] #used for both original df and new df_add\n",
    "print(df_Col_keep)\n",
    "merge_col = \"doi link\" #choose the column that will be the reference for merging\n",
    "concatenate_cols = [\"term\"] #choose column(s) that will be concatenated (the others will be combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#making sure that empty or None in merge_col take value from another specific col\n",
    "def merge_col_empty_fill(df, merge_col, unique_col, string_clean):    \n",
    "    for index, row in df.iterrows():\n",
    "        doi_link = row['doi link']\n",
    "        if pd.isnull(doi_link):\n",
    "            doi_link = ''\n",
    "        if doi_link == '':\n",
    "            x_unique = f\"Unknown DOI {df.loc[index, unique_col].replace(string_clean,'')}\"\n",
    "            df.loc[index, merge_col] = x_unique\n",
    "            print(f\"Replaced null for {x_unique}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_col = 'path'\n",
    "string_clean = \"../papers-xml-extractions/Vis-all_full_paper_pdfs-text-extraction-results/\"\n",
    "df = merge_col_empty_fill(df, merge_col, unique_col, string_clean)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_col = 'path'\n",
    "string_clean = \"../papers-xml-extractions/run 3 - vis 2022 full + vis 2021 conf/\"\n",
    "df_add = merge_col_empty_fill(df_add, merge_col, unique_col, string_clean)\n",
    "df_add.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#adding missing part to transform DOI into URL\n",
    "doi_URL = 'http://dx.doi.org/'\n",
    "link_clean_needed = False\n",
    "\n",
    "def add_doi_url(row):\n",
    "    \n",
    "    #the list of checks to perform\n",
    "    def check_doi_content(row):\n",
    "        #change the nuber of checks if you add one\n",
    "        nb_of_checks = 5\n",
    "        \n",
    "        global check_list\n",
    "        check_list = [False]*nb_of_checks\n",
    "        \n",
    "        global i\n",
    "        i = 0\n",
    "        \n",
    "        def to_True():\n",
    "            global check_list\n",
    "            global i\n",
    "            check_list[i]=True\n",
    "            i+=1\n",
    "            \n",
    "        \n",
    "        #checks to make\n",
    "        if isinstance(row, str):\n",
    "            to_True()\n",
    "            \n",
    "            if row != '':\n",
    "                to_True()\n",
    "                            \n",
    "            if 'Unknown' not in row:\n",
    "                to_True()\n",
    "                \n",
    "            ##second to last check\n",
    "            if row.count(\"http://dx.doi.org/\")==0:\n",
    "                to_True()\n",
    "                \n",
    "            #last check\n",
    "            if row.count(\"http://dx.doi.org/\")<=1:\n",
    "                check_list[i]=False\n",
    "            else:\n",
    "                to_True()\n",
    "        \n",
    "        else:\n",
    "            print(\"Err: not str\")\n",
    "            print(type(row))\n",
    "        \n",
    "        return check_list\n",
    "       \n",
    "    checks_done = check_doi_content(row)\n",
    "    \n",
    "    doi_URL_there = True #assuming there is a DOI link\n",
    "    \n",
    "    if checks_done[-2]:\n",
    "        print(\"DOI was alone\")\n",
    "        doi_URL_there = False\n",
    "    \n",
    "    #removing http://dx.doi.org/ when multiple\n",
    "    if not checks_done[-1]:\n",
    "        global link_clean_needed\n",
    "        link_clean_needed = True\n",
    "        del checks_done[-1]\n",
    "    \n",
    "    #adding\n",
    "    if all(checks_done) and not doi_URL_there:\n",
    "        row = doi_URL+row\n",
    "        print(\"DOI was alone, we added http://dx.doi.org/\")\n",
    "        \n",
    "    return row\n",
    "\n",
    "    \n",
    "def link_clean(df, doi_col):\n",
    "    df[doi_col] = df[doi_col].apply(lambda x: add_doi_url(x))\n",
    "    global link_clean_needed\n",
    "    if link_clean_needed:\n",
    "        print(f'Some links have multiple \"{doi_URL}\" in URL for this df {df.info()} Please use the next line or manually replace \"{doi_URL}{doi_URL}\" with \"{doi_URL}\".')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = link_clean(df, merge_col)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_add = link_clean(df_add, merge_col)\n",
    "df_add.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#if needed to replace some erroneous strings...\n",
    "df['doi link']=df['doi link'].str.replace(\"http://dx.doi.org/http://dx.doi.org/\",\"http://dx.doi.org/\", regex=True)\n",
    "df_add['doi link']=df_add['doi link'].str.replace(\"http://dx.doi.org/http://dx.doi.org/\",\"http://dx.doi.org/\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#lists of cols to drop for each df\n",
    "df_add_Col_drop = [x for x in df_add_Cols if x not in df_Col_keep]\n",
    "df_Col_drop = [x for x in df_Cols if x not in df_Col_keep]\n",
    "\n",
    "print(f\"df delete {df_Col_drop}\")\n",
    "print(f\"df_add delete {df_add_Col_drop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_Cols(df):\n",
    "    df_Cols = df.columns.values.tolist()\n",
    "    for n in df_Cols:\n",
    "        if n not in df_Col_keep:\n",
    "            print(df.columns.values.tolist())\n",
    "            df.drop(n, inplace=True, axis=1)\n",
    "            print(f\"Removed {n} column\")\n",
    "    df = df[df_Col_keep]\n",
    "    df.info() #arrange col order\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = drop_Cols(df)\n",
    "df_add = drop_Cols(df_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#check\n",
    "if df.columns.values.tolist() != df_add.columns.values.tolist():\n",
    "    raise ValueError(\"The keep list and final col list don't match.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df_add.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_columns(df):\n",
    "    #columns to be combined\n",
    "    combine_cols = [x for x in df_Col_keep if x not in concatenate_cols]\n",
    "    combine_cols.remove(merge_col)\n",
    "    print(combine_cols)\n",
    "    #combine same columns\n",
    "    for n in combine_cols :\n",
    "        try:\n",
    "            df[f'{n}_x'] = df[f'{n}_x'].combine_first(df[f'{n}_y']) #combining _x (left from merging) and _y (right from merging) columns into _x\n",
    "            df.rename(columns = {f'{n}_x':f'{n}'}, inplace=True) #rename column _x\n",
    "            del df[f'{n}_y'] #dropping the _y column\n",
    "        except Exception as e:\n",
    "            print(f\"!! Couldn't combine columns {n}, error {e}\")\n",
    "\n",
    "    #building continued index for new elements\n",
    "\n",
    "    global df_max_index\n",
    "\n",
    "    df_max_index += 1\n",
    "    df.info()\n",
    "    df[\"index\"] = df[\"index\"].apply(lambda x: 0 if x is np.nan else x)\n",
    "\n",
    "    def addIndex(row):\n",
    "        global df_max_index\n",
    "        if row == 0:\n",
    "            row = df_max_index\n",
    "            df_max_index += 1\n",
    "        return row\n",
    "    \n",
    "    df[\"index\"] = df[\"index\"].apply(lambda x: addIndex(x))\n",
    "    \n",
    "    #concatenating\n",
    "    for m in concatenate_cols:\n",
    "        ## replacing NaN with empty strings first\n",
    "        df[f'{m}_x'] = df[f'{m}_x'].apply(lambda x: '' if x is np.nan else x)\n",
    "        df[f'{m}_y'] = df[f'{m}_y'].apply(lambda x: '' if x is np.nan else x)\n",
    "        ## concatenating terms & cleaning columns\n",
    "        try :\n",
    "            df[f'{m}_x'] = df[f'{m}_x'] + df[f'{m}_y']    \n",
    "        except:\n",
    "            print(f\"Error on column {m}\")\n",
    "        df.rename(columns = {f'{m}_x':f'{m}'}, inplace=True)\n",
    "        del df[f'{m}_y']\n",
    "    \n",
    "    print(df_max_index)\n",
    "    df['index'] = df['index'].astype('int')\n",
    "    df['term'] = df['term'].astype('str')\n",
    "    df.info()\n",
    "    return df\n",
    "\n",
    "#merging 2 dataframess at a time\n",
    "def merge_dfs(df_x, df_y):\n",
    "    print(f\"merging based on '{merge_col}' column\")\n",
    "    df = pd.merge(\n",
    "        df_x, #left source\n",
    "        df_y, #right source\n",
    "        how=\"outer\",\n",
    "        on=merge_col,\n",
    "        left_on=None,\n",
    "        right_on=None,\n",
    "        left_index=False,\n",
    "        right_index=False,\n",
    "        sort=False,\n",
    "        suffixes=('_x', '_y'), #used to identify left and right sources in temp df\n",
    "        copy=True,\n",
    "        indicator=True\n",
    "    )\n",
    "    df.to_csv(f\"{results_path}/{csv_out}_temp_with_duplicates.csv\",index=False)\n",
    "    df.drop_duplicates(subset=[merge_col], keep='first', inplace=True, ignore_index=True)\n",
    "    df.info()\n",
    "    df.to_csv(f\"{results_path}/{csv_out}_temp_without_duplicates.csv\",index=False)\n",
    "    df = combine_columns(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#merging\n",
    "final_df=merge_dfs(df,df_add)\n",
    "\n",
    "#cleaning years\n",
    "final_df['year']=final_df['year'].apply(lambda y: 0 if np.isnan(y) else int(y))\n",
    "\n",
    "print(final_df)\n",
    "\n",
    "#droping the _merge column, not useful if everything went ok\n",
    "#final_df.drop(columns='_merge', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#manual conformity with destination column order\n",
    "final_df=final_df[['index','path','filename','title','year','doi link','term','foundText','_merge']]\n",
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#creating output file\n",
    "final_df.to_csv(f\"{results_path}/{csv_out}.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
