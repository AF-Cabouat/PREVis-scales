{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "h6soY6SdNvcw",
    "tags": []
   },
   "source": [
    "# Search the VIS paper xmls for keywords\n",
    "\n",
    "The xml files can be found in all subfolders here and include full text extraction results from the grobid tool.\n",
    "\n",
    "Authors: Petra Isenberg, Tingying He\n",
    "Date: October 2021\n",
    "\n",
    "Edit cell 5 Anne-Flore Cabouat - Mar 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1yKUVCkNvc1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#First we load a couple of required libraries\n",
    "\n",
    "from lxml import etree\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import csv\n",
    "from shutil import copyfile\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQnlh01YNvc2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "teins = {'tei':'http://www.tei-c.org/ns/1.0'} #info on the xml structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#path of the xml files\n",
    "path = \"../papers-xml-extractions/\"\n",
    "\n",
    "#csv output path (results)\n",
    "results_path = \"../1-keywords-literature-search-results/\"\n",
    "\n",
    "#if you're doing multiple rounds (different xml sources dir for same term), if not keep ''\n",
    "round_nb = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62MF289rNvc3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#variables we will retrieve from XML files to build a dataframe\n",
    "titleColumn = []\n",
    "yearColumn = []\n",
    "pathColumn = []\n",
    "filenameColumn = []\n",
    "doiColumn = []\n",
    "searchtermColumn = []\n",
    "conferenceColumn = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "caIT-IgYNvc3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#our XML files are stored in directories with naming convention: conference-year\n",
    "\n",
    "#Function that will extract the conference name out of the folder name\n",
    "def findConferenceName(dirpath):\n",
    "    if 'InfoVis' in dirpath:\n",
    "        return 'InfoVis'\n",
    "    elif 'Vast' in dirpath:\n",
    "        return 'Vast'\n",
    "    elif 'SciVis' in dirpath:\n",
    "        return 'SciVis'\n",
    "    elif 'Vis-' in dirpath:\n",
    "        return 'Vis'\n",
    "    else:\n",
    "        return 'not found'\n",
    "    \n",
    "#function that will extract the year out of the folder name\n",
    "def findYear(dirpath):\n",
    "    return dirpath[-4:]\n",
    "\n",
    "#function that will find the year in DOI\n",
    "years = [*range(1990, 2023, 1)]\n",
    "def findYearFromDOI(doi, years):\n",
    "    for year in years:\n",
    "        if doi.find(f'.{year}.') != -1:\n",
    "            YearFound = year\n",
    "            return YearFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#function called to drop duplicates files found\n",
    "def dup_drop(df, subset_str):\n",
    "    len_before = len(df)\n",
    "    df.drop_duplicates(subset=[subset_str], keep='last', inplace=True, ignore_index=True) #droping duplicates\n",
    "    len_after = len(df)\n",
    "    print(f'Dropped {len_before} duplicates by {subset_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZCdZjk9PNvc4",
    "outputId": "5308c154-3b51-4f68-e31a-df8f351c33a3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#let's walk through all files\n",
    "def search_term(searchterm):\n",
    "    #let's walk through all files\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for filename in [f for f in filenames if f.endswith(\".xml\")]:\n",
    "            firstTime = 1\n",
    "            searchtermText = \"\"\n",
    "\n",
    "            conference = findConferenceName(dirpath)\n",
    "            year = findYear(dirpath)\n",
    "\n",
    "            filepath = dirpath + \"/\" + filename\n",
    "            \n",
    "            dirname = dirpath.replace(path, '')\n",
    "\n",
    "            with open(filepath,'r', encoding='utf8') as file:\n",
    "                try:\n",
    "                    #first we check that we have information to identify the paper\n",
    "                    tree = etree.parse(file)\n",
    "                    root = etree.XML(etree.tostring(tree))\n",
    "                    teiheader = root.find(\".//tei:teiHeader\",teins)\n",
    "                    textNode = root.find(\".//tei:text\",teins)\n",
    "\n",
    "                    #do we have a readable title?\n",
    "                    title = teiheader.find(\".//tei:title\",teins)\n",
    "                    if title is None or title =='':\n",
    "                        title = f\"Unknown title {dirname}/{filename}\" #\"unique\" str to avoid duplicate check errors\n",
    "                    if \"Ã—Ã˜ ÃœÃ˜Ã\" in title:\n",
    "                        print(\"Probably error with file: \" + filepath)\n",
    "                    elif \"@@@@\" in title:\n",
    "                        print(\"Probably error with file: \" + filepath)\n",
    "\n",
    "                    #do we have a DOI?\n",
    "                    idno = teiheader.findall(\".//tei:idno\",teins)\n",
    "                    doi = f\"Unknown DOI {dirname}/{filename}\" #\"unique\" str to avoid duplicate check errors\n",
    "                    for i in idno:\n",
    "                        if i.get(\"type\") == \"DOI\":\n",
    "                            doi = i.text\n",
    "                            break\n",
    "                    #attempt to retrieve year from DOI if it wasn't in dir name (old DOIs)\n",
    "                    if not year.isnumeric():\n",
    "                        year = findYearFromDOI(doi, years)\n",
    "                        \n",
    "                    print(\"Working on file: \" + filename)\n",
    "\n",
    "                    for elem in textNode.iter():\n",
    "                        if elem.text: \n",
    "                            if searchterm.lower() in elem.text.lower():\n",
    "                                searchtermText = searchtermText + \"<\" + elem.tag.replace(\"{http://www.tei-c.org/ns/1.0}\",\"\") + \">\" + elem.text + \"+\"\n",
    "                                #if the search term is found, do this\n",
    "\n",
    "                                if firstTime == 1: #When the searchterm is found in a paper for the first time, do this\n",
    "                                    print(doi)\n",
    "                                    yearColumn.append(year)\n",
    "                                    titleColumn.append(title.text)\n",
    "                                    doiColumn.append(doi)\n",
    "                                    pathColumn.append(filepath)\n",
    "                                    filenameColumn.append(filename.replace(\".tei.xml\",\"\"))\n",
    "                                    firstTime = 0\n",
    "                        if elem.tail: #check whether searchterm is in text after a closing tag\n",
    "                            if searchterm.lower() in elem.tail.lower():\n",
    "                                searchtermText = searchtermText + \"<\" + elem.tag.replace(\"{http://www.tei-c.org/ns/1.0}\",\"\") + \">\" + elem.tail + \"+\"\n",
    "                                #if the search term is found, do this\n",
    "\n",
    "                                if firstTime == 1: #When the searchterm is found in a paper for the first time, do this\n",
    "                                    yearColumn.append(year)\n",
    "                                    titleColumn.append(title.text)\n",
    "                                    doiColumn.append(doi)\n",
    "                                    pathColumn.append(filepath)\n",
    "                                    filenameColumn.append(filename.replace(\".tei.xml\",\"\"))\n",
    "                                    firstTime = 0\n",
    "\n",
    "                    if searchtermText: # If the searchterm is found in this paper, do this\n",
    "                        searchtermColumn.append(searchtermText)\n",
    "\n",
    "\n",
    "\n",
    "                except Exception as e: # work on python 3.x\n",
    "                    print(str(e))\n",
    "                    #rows.append([\"ERROR\",file,\"ERROR\"])\n",
    "                    #copyErrorFiles(filepath)\n",
    "\n",
    "    data = {'path':pathColumn,\n",
    "            'filename':filenameColumn,\n",
    "            'title':titleColumn,\n",
    "            'doi link':doiColumn,\n",
    "            'year':yearColumn,\n",
    "            'foundText':searchtermColumn,\n",
    "            'conference': conferenceColumn\n",
    "            }\n",
    "\n",
    "    print(str(len(filenameColumn))+\" and \"+str(len(searchtermColumn)))\n",
    "\n",
    "    df = pd.DataFrame(data) \n",
    "    df[f\"term\"] = searchterm+\"; \" #modified to facilitate merging dfs, keeping track of terms (AFC)\n",
    "    \n",
    "    #droping duplicates \n",
    "    dup_drop(df, 'foundText')\n",
    "    dup_drop(df, 'path')\n",
    "    dup_drop(df, 'title')\n",
    "    \n",
    "    #if there are duplicates left, store them for manual review\n",
    "    dup_df = df[df.duplicated(subset=['filename'],keep=False)]\n",
    "    if len(dup_df) > 0:\n",
    "        print(f\"\\nRemaining duplicates by filename: {len(dup_df)}\")\n",
    "        dup_df.to_csv(f\"{results_path}/paperFoundWith-\"+searchterm+round_nb+\"DUPLICATES.csv\",index=False)\n",
    "    else:\n",
    "        print(\"No duplicates remaining based on filename\")\n",
    "    \n",
    "    df.to_csv(f\"{results_path}/paperFoundWith-\"+searchterm+round_nb+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run one by one. Restart kernel or clear memory before running a new line\n",
    "\n",
    "#search_term('readab')\n",
    "#search_term('legib')\n",
    "#search_term('likert')\n",
    "search_term('deciph')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cleaning remaining duplicates\n",
    "Different papers may have the same filename across folders without a distinctive DOI - in which case those files have been stored in dedicated csv files for each term searched.\n",
    "\n",
    "Please manually go through -DUPLICATES.csv files in the results forlder to review them \n",
    "    > and clean the corresponding paperFoundwith*searchterm*.csv file\n",
    "\n",
    "Then you can move on to the next part if needed (if you have mutliple terms) = merging multiple output csv files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RV2ScXw0Nvc5",
    "tags": []
   },
   "source": [
    "# Merging the results\n",
    "\n",
    "Author: Anne-Flore Cabouat\n",
    "Date: March 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kVyGRetxNvc6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Yh4qdhDONvc6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "##please change your local settings here\n",
    "\n",
    "## input folder (default = previous step's out folder)\n",
    "files_path = results_path\n",
    "\n",
    "#output (defaut = same as input)\n",
    "out_path = files_path\n",
    "out_name = 'paperFound-MERGED'\n",
    "\n",
    "#list the files you want to merge\n",
    "#!! they all need to have the exact same column structure\n",
    "files_dict = {\n",
    "    #!! please double check that all keys are unique and represent a range of numbers starting from 1\n",
    "    '1' : \"paperFoundWith-readab.csv\",\n",
    "    '2' : \"paperFoundWith-legib.csv\",\n",
    "    '3' : \"paperFoundWith-likert.csv\",\n",
    "    '4' : \"paperFoundWith-deciph.csv\",\n",
    "}\n",
    "#warning for duplicate csv files in the dict\n",
    "if len(files_dict) != len(set(files_dict.values())):\n",
    "    raise ValueError(\"The dictionary contains repetition in filenames, please check again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Yh4qdhDONvc6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's read the files\n",
    "def read_csvs(source_dict):\n",
    "    #listing files\n",
    "    files_list = []\n",
    "    files_count = len(list(files_dict.keys()))\n",
    "    print(f\"{files_count} files listed\")\n",
    "    global files_range\n",
    "    files_range = [*range(1,files_count+1,1)] #!! please double check that all keys are unique and represent a range of numbers starting from 1\n",
    "    print(f\"Range of files: {files_range}\")\n",
    "    #list of full file paths\n",
    "    for k, f in files_dict.items():\n",
    "        files_list.append(f\"{files_path}/{f}\")\n",
    "    #reading csv and storing dfs in the global dict\n",
    "    global df_dict\n",
    "    try:\n",
    "        df_dict = {i: pd.read_csv(files_list[i-1]) for i in files_range}\n",
    "    except Exception as e:\n",
    "        print(f\"The function passed with an error: {str(e)}\")\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "read_csvs(files_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Yh4qdhDONvc6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['path', 'filename', 'title', 'doi link', 'year', 'foundText', 'term']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 0 entries\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   path       0 non-null      object\n",
      " 1   filename   0 non-null      object\n",
      " 2   title      0 non-null      object\n",
      " 3   doi link   0 non-null      object\n",
      " 4   year       0 non-null      object\n",
      " 5   foundText  0 non-null      object\n",
      " 6   term       0 non-null      object\n",
      "dtypes: object(7)\n",
      "memory usage: 0.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "#column structure in a list\n",
    "df_columns_list_full = df_dict[1].columns.values.tolist()\n",
    "print(df_columns_list_full)\n",
    "\n",
    "column_merge = 'path' #this columns will be used for identifying duplicates during the merge\n",
    "df_columns_saved = ['foundText', 'term'] #these columns will be concatenated\n",
    "df_columns_list = [item for item in df_columns_list_full if item not in df_columns_saved] #these columns will be combined_first\n",
    "df_columns_list.remove(column_merge)\n",
    "dup_drop = 'foundText' #how duplicates will be identified and droped (best on foundText)\n",
    "dup_check = 'doi link' #to check for possible remaining duplicates after the drop\n",
    "\n",
    "#intializing first df\n",
    "df_merged = pd.DataFrame(columns=df_columns_list_full)\n",
    "df_dup = df_merged\n",
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yR9dLeETNvc7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#clean columns after merging\n",
    "def combine_columns(df):\n",
    "    #combine_first = if left has valuem keep that - else take value from right\n",
    "    for n in df_columns_list : #leaving out first col (path, used to merge) and last col (term, we need to keep both)\n",
    "        try:\n",
    "            df[f'{n}_x'] = df[f'{n}_x'].combine_first(df[f'{n}_y']) #combining _x (left from merging) and _y (right from merging) columns into _x\n",
    "            df.rename(columns = {f'{n}_x':f'{n}'}, inplace=True) #rename column _x\n",
    "            del df[f'{n}_y'] #dropping the _y column\n",
    "        except Exception as e:\n",
    "            print(f\"!! Couldn't combine columns {n}, error {e}\")\n",
    "            \n",
    "    # concatenate right value to left one\n",
    "    for m in df_columns_saved:\n",
    "        ## replacing NaN with empty strings first\n",
    "        df[f'{m}_x'] = df[f'{m}_x'].apply(lambda x: '' if x is np.nan else x)\n",
    "        df[f'{m}_y'] = df[f'{m}_y'].apply(lambda x: '' if x is np.nan else x)\n",
    "        ## concatenating terms & cleaning columns\n",
    "        df[f'{m}_x'] = df[f'{m}_x'] + df[f'{m}_y']\n",
    "        df.rename(columns = {f'{m}_x':f'{m}'}, inplace=True)\n",
    "        del df[f'{m}_y']\n",
    "    \n",
    "    #droping duplicates & identifying possible remaning ones\n",
    "    len_before = len(df)\n",
    "    df.drop_duplicates(subset=[dup_drop], keep='last', inplace=True, ignore_index=True)\n",
    "    len_after = len(df)\n",
    "    print(f'Dropped {len_before} duplicates by {dup_drop}')\n",
    "\n",
    "    print(\"\\n************\\nAFTER DUPLICATES CLEANED\\n\")\n",
    "    df.info()\n",
    "    \n",
    "    global df_dup\n",
    "    df_dup = pd.concat([df_dup, df[df.duplicated(subset=[dup_check],keep=False)]], ignore_index=True)\n",
    "    if len(df_dup) > 0:\n",
    "        print(f\"\\n!!! duplicates remaining based on {dup_check}: {len(df_dup)}.\\n The full df_dup dataframe should be saved as csv.\")\n",
    "    else:\n",
    "        print(f\"\\n No duplicates remaining based on {dup_check}\\n\")\n",
    "\n",
    "    global df_merged\n",
    "    df_merged = df\n",
    "    return df\n",
    "\n",
    "merge_count = 0\n",
    "\n",
    "#merging 2 dataframess at a time\n",
    "def merge_dfs(df_x, df_y):\n",
    "    global merge_count\n",
    "    merge_count += 1\n",
    "    print(f\"merging based on '{column_merge}' column\")\n",
    "    global df_merged\n",
    "    df_merged = pd.merge(\n",
    "        df_x, #left source\n",
    "        df_y, #right source\n",
    "        how=\"outer\",\n",
    "        on=column_merge,\n",
    "        left_on=None,\n",
    "        right_on=None,\n",
    "        left_index=False,\n",
    "        right_index=False,\n",
    "        sort=True,\n",
    "        suffixes=(\"_x\", \"_y\"), #used to identify left and right sources in final df\n",
    "        copy=True,\n",
    "        indicator=False,\n",
    "        validate=None,\n",
    "    )\n",
    "    print(\"\\n************\\nAFTER MERGE\\n\")\n",
    "    df_merged.info()\n",
    "    df_merged.to_csv(f\"{out_path}/paperFound-MERGED+DOI_temp{merge_count}.csv\",index=False)\n",
    "    print(\"\\n\\n\\n\\n\\n\\n\")\n",
    "    combine_columns(df_merged)\n",
    "    return df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging based on 'path' column\n",
      "\n",
      "************\n",
      "AFTER MERGE\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 84 entries, 0 to 83\n",
      "Data columns (total 13 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   filename_x   0 non-null      object \n",
      " 1   title_x      0 non-null      object \n",
      " 2   doi link_x   0 non-null      object \n",
      " 3   year_x       0 non-null      object \n",
      " 4   foundText_x  0 non-null      object \n",
      " 5   term_x       0 non-null      object \n",
      " 6   path         84 non-null     object \n",
      " 7   filename_y   84 non-null     object \n",
      " 8   title_y      84 non-null     object \n",
      " 9   doi link_y   84 non-null     object \n",
      " 10  year_y       45 non-null     float64\n",
      " 11  foundText_y  84 non-null     object \n",
      " 12  term_y       84 non-null     object \n",
      "dtypes: float64(1), object(12)\n",
      "memory usage: 9.2+ KB\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dropped 84 duplicates by foundText\n",
      "\n",
      "************\n",
      "AFTER DUPLICATES CLEANED\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 84 entries, 0 to 83\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   filename   84 non-null     object\n",
      " 1   title      84 non-null     object\n",
      " 2   doi link   84 non-null     object\n",
      " 3   year       45 non-null     object\n",
      " 4   foundText  84 non-null     object\n",
      " 5   term       84 non-null     object\n",
      " 6   path       84 non-null     object\n",
      "dtypes: object(7)\n",
      "memory usage: 4.7+ KB\n",
      "\n",
      " No duplicates remaining based on doi link\n",
      "\n",
      "merging based on 'path' column\n",
      "\n",
      "************\n",
      "AFTER MERGE\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 94 entries, 0 to 93\n",
      "Data columns (total 13 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   filename_x   84 non-null     object \n",
      " 1   title_x      84 non-null     object \n",
      " 2   doi link_x   84 non-null     object \n",
      " 3   year_x       45 non-null     object \n",
      " 4   foundText_x  84 non-null     object \n",
      " 5   term_x       84 non-null     object \n",
      " 6   path         94 non-null     object \n",
      " 7   filename_y   21 non-null     object \n",
      " 8   title_y      21 non-null     object \n",
      " 9   doi link_y   21 non-null     object \n",
      " 10  year_y       12 non-null     float64\n",
      " 11  foundText_y  21 non-null     object \n",
      " 12  term_y       21 non-null     object \n",
      "dtypes: float64(1), object(12)\n",
      "memory usage: 10.3+ KB\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dropped 94 duplicates by foundText\n",
      "\n",
      "************\n",
      "AFTER DUPLICATES CLEANED\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 94 entries, 0 to 93\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   filename   94 non-null     object\n",
      " 1   title      94 non-null     object\n",
      " 2   doi link   94 non-null     object\n",
      " 3   year       51 non-null     object\n",
      " 4   foundText  94 non-null     object\n",
      " 5   term       94 non-null     object\n",
      " 6   path       94 non-null     object\n",
      "dtypes: object(7)\n",
      "memory usage: 5.3+ KB\n",
      "\n",
      " No duplicates remaining based on doi link\n",
      "\n",
      "merging based on 'path' column\n",
      "\n",
      "************\n",
      "AFTER MERGE\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 140 entries, 0 to 139\n",
      "Data columns (total 13 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   filename_x   94 non-null     object \n",
      " 1   title_x      94 non-null     object \n",
      " 2   doi link_x   94 non-null     object \n",
      " 3   year_x       51 non-null     object \n",
      " 4   foundText_x  94 non-null     object \n",
      " 5   term_x       94 non-null     object \n",
      " 6   path         140 non-null    object \n",
      " 7   filename_y   71 non-null     object \n",
      " 8   title_y      71 non-null     object \n",
      " 9   doi link_y   71 non-null     object \n",
      " 10  year_y       31 non-null     float64\n",
      " 11  foundText_y  71 non-null     object \n",
      " 12  term_y       71 non-null     object \n",
      "dtypes: float64(1), object(12)\n",
      "memory usage: 15.3+ KB\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dropped 140 duplicates by foundText\n",
      "\n",
      "************\n",
      "AFTER DUPLICATES CLEANED\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 140 entries, 0 to 139\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   filename   140 non-null    object\n",
      " 1   title      140 non-null    object\n",
      " 2   doi link   140 non-null    object\n",
      " 3   year       70 non-null     object\n",
      " 4   foundText  140 non-null    object\n",
      " 5   term       140 non-null    object\n",
      " 6   path       140 non-null    object\n",
      "dtypes: object(7)\n",
      "memory usage: 7.8+ KB\n",
      "\n",
      " No duplicates remaining based on doi link\n",
      "\n",
      "merging based on 'path' column\n",
      "\n",
      "************\n",
      "AFTER MERGE\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 166 entries, 0 to 165\n",
      "Data columns (total 13 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   filename_x   140 non-null    object \n",
      " 1   title_x      140 non-null    object \n",
      " 2   doi link_x   140 non-null    object \n",
      " 3   year_x       70 non-null     object \n",
      " 4   foundText_x  140 non-null    object \n",
      " 5   term_x       140 non-null    object \n",
      " 6   path         166 non-null    object \n",
      " 7   filename_y   91 non-null     object \n",
      " 8   title_y      91 non-null     object \n",
      " 9   doi link_y   91 non-null     object \n",
      " 10  year_y       46 non-null     float64\n",
      " 11  foundText_y  91 non-null     object \n",
      " 12  term_y       91 non-null     object \n",
      "dtypes: float64(1), object(12)\n",
      "memory usage: 18.2+ KB\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dropped 166 duplicates by foundText\n",
      "\n",
      "************\n",
      "AFTER DUPLICATES CLEANED\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 166 entries, 0 to 165\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   filename   166 non-null    object\n",
      " 1   title      166 non-null    object\n",
      " 2   doi link   166 non-null    object\n",
      " 3   year       87 non-null     object\n",
      " 4   foundText  166 non-null    object\n",
      " 5   term       166 non-null    object\n",
      " 6   path       166 non-null    object\n",
      "dtypes: object(7)\n",
      "memory usage: 9.2+ KB\n",
      "\n",
      " No duplicates remaining based on doi link\n",
      "\n",
      "merging based on 'path' column\n",
      "\n",
      "************\n",
      "AFTER MERGE\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 221 entries, 0 to 220\n",
      "Data columns (total 13 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   filename_x   166 non-null    object \n",
      " 1   title_x      166 non-null    object \n",
      " 2   doi link_x   166 non-null    object \n",
      " 3   year_x       87 non-null     object \n",
      " 4   foundText_x  166 non-null    object \n",
      " 5   term_x       166 non-null    object \n",
      " 6   path         221 non-null    object \n",
      " 7   filename_y   154 non-null    object \n",
      " 8   title_y      153 non-null    object \n",
      " 9   doi link_y   154 non-null    object \n",
      " 10  year_y       77 non-null     float64\n",
      " 11  foundText_y  154 non-null    object \n",
      " 12  term_y       154 non-null    object \n",
      "dtypes: float64(1), object(12)\n",
      "memory usage: 24.2+ KB\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dropped 221 duplicates by foundText\n",
      "\n",
      "************\n",
      "AFTER DUPLICATES CLEANED\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 221 entries, 0 to 220\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   filename   221 non-null    object\n",
      " 1   title      220 non-null    object\n",
      " 2   doi link   221 non-null    object\n",
      " 3   year       111 non-null    object\n",
      " 4   foundText  221 non-null    object\n",
      " 5   term       221 non-null    object\n",
      " 6   path       221 non-null    object\n",
      "dtypes: object(7)\n",
      "memory usage: 12.2+ KB\n",
      "\n",
      " No duplicates remaining based on doi link\n",
      "\n",
      "merging based on 'path' column\n",
      "\n",
      "************\n",
      "AFTER MERGE\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 221 entries, 0 to 220\n",
      "Data columns (total 13 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   filename_x   221 non-null    object \n",
      " 1   title_x      220 non-null    object \n",
      " 2   doi link_x   221 non-null    object \n",
      " 3   year_x       111 non-null    object \n",
      " 4   foundText_x  221 non-null    object \n",
      " 5   term_x       221 non-null    object \n",
      " 6   path         221 non-null    object \n",
      " 7   filename_y   6 non-null      object \n",
      " 8   title_y      6 non-null      object \n",
      " 9   doi link_y   6 non-null      object \n",
      " 10  year_y       2 non-null      float64\n",
      " 11  foundText_y  6 non-null      object \n",
      " 12  term_y       6 non-null      object \n",
      "dtypes: float64(1), object(12)\n",
      "memory usage: 24.2+ KB\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dropped 221 duplicates by foundText\n",
      "\n",
      "************\n",
      "AFTER DUPLICATES CLEANED\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 221 entries, 0 to 220\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   filename   221 non-null    object\n",
      " 1   title      220 non-null    object\n",
      " 2   doi link   221 non-null    object\n",
      " 3   year       111 non-null    object\n",
      " 4   foundText  221 non-null    object\n",
      " 5   term       221 non-null    object\n",
      " 6   path       221 non-null    object\n",
      "dtypes: object(7)\n",
      "memory usage: 12.2+ KB\n",
      "\n",
      " No duplicates remaining based on doi link\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in files_range:\n",
    "    try:\n",
    "        merge_dfs(df_merged, df_dict[i])\n",
    "    except Exception as e:\n",
    "        print(f\"\\n *** An error occurred: {str(e)} *** \\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#output files\n",
    "df_merged.to_csv(f\"{out_path}/{out_name}.csv\",index=False)\n",
    "if len(df_dup) > 0:\n",
    "    df_dup.to_csv(f\"{out_path}/{out_name}-remaining-duplicates.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Options for printing if you want intermediary checks in the functions\n",
    "#pd.set_option('display.min_rows', 30)\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_colwidth', 25)\n",
    "\n",
    "#or use print with:\n",
    "#with pd.option_context('display.min_rows', 30, 'display.max_columns', None, 'display.max_colwidth', 25):\n",
    "#    print(df) \"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
