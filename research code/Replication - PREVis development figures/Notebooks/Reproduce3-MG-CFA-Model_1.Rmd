---
title: "Reliability and CFA Multi-levels analysis - Full data and Model_1"
author: "Anne-Flore Cabouat"
date: "2024-01-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#set working directory
# my_dir <- "your_data_directory" #eg: "absolute_path_to/Supplementary-material-folder/2 - Scale development/"
knitr::opts_knit$set(root.dir = paste(my_dir, "Results/Data_Analysis", sep=""))

```

## Librairies

```{r librairies}
library(dplyr)
library(knitr) #RStudio
library(png) #output png
library(corrplot) #plot correlation matrix
library(misty) #Little's test for MCAR
library(mifa) # MICE for missing data handling
library(psych) # for EFA
library(RColorBrewer) #for corr matrix
library(lavaan) # for cfa
```
## Functions
```{r functions}

make_corrM <- function(d, order = NULL, nb_fa = 4){
  # Multiple imputation using Fully Conditional Specification (FCS) implemented by the MICE algorithm as described in Van Buuren and Groothuis-Oudshoorn (2011) doi:10.18637/jss.v045.i03
  #run MICE and generate covariance matrix with with mifa package
  mi <- invisible(mifa(
  data      = d, 
  n_pc      = nb_fa, #factors estimates
  ci        = "fieller", #confidence interval method
  print     = FALSE,
  m = 5 #number of MICE iterations
  ))
  
  corr_MICE <- cov2cor(mi$cov_combined)
  
  if (length(order)>0){
    corrM_reorder <- corr_MICE[order, order]
    corr_MICE <- corrM_reorder
  }
  
  # corrM <- corFiml(d)
  return(corr_MICE)
}
  

# Applicability of EFA

## Bartlett test of sphericity - from TH
bartlettTest <- function(corrMatrix,data){
  bart <- cortest.bartlett(corrMatrix, n = nrow(data))
  if(bart[2]>0.05) cat("WARNING the p value is above 0.05") else cat("The p value is below 0.05. We are good to continue.")
  cat("\n\n")
  print(bart)
  return(bart)
}

##KMO to complement Bartlett - from TH
KMOTest <- function(corrMatrix){
  kmo <- KMO(corrMatrix)
  cat(paste("The overall measure of sampling adequacy is: ",kmo[1]))
  cat("\n\n")
  if(kmo[1]<.7) cat("WARNING the sampling adequacy has dropped below 0.7") else cat("The sampling adequacy is above 0.7. We are generally good.")
  cat("\n\n")
  return(kmo)
}

# Missing values

missingTest <- function(na_percent, mcar_pvalue){
  if (na_percent<5 & mcar_pvalue<=.05){
      cat("Data is MCAR and less than 5% missing data, which is negligible. We are good to continue.")
  } else if (na_percent<5 & mcar_pvalue>.05){
      cat("Data is NOT MCAR but less than 5% missing data, which is negligible. We are good to continue.")
  } else if(na_percent>=5 & mcar_pvalue>.05) {
      cat("WARNING missing data is not negligible (> 5%) AND missing data is NOT MCAR. Further investigation is needed.")
    }else if (na_percent>=5 & na_percent<40 & mcar_pvalue<=.05){
      cat("Data is MCAR but more than 5% missing data. Use imputation / likelihood methods.")
    }  else {
      cat("40% or more missing data and / or something is wrong.")
    }}


# Parallel analysis for each group

parallelAnalysis <- function(corrMatrix, observations = nrow(the_data), file_suffix){
  pdf(paste(paste("generatedPlots-EFA/ScreePlot-",file_suffix,sep=""),'.pdf'), width=8, height=4)
  parallel <- fa.parallel(corrMatrix, n.obs=observations, fa="fa", n.iter=100, main="Scree plots with parallel analysis", error.bars = TRUE)
  print(parallel)
  dev.off() 
  cat("\n\n")
}

stimulus_data_test <- function(the_data, the_letter){
  ## Here wwe run test on the data
  ### Shapiro Wilk’s test (P-values < 0.05 indicate not normal)
  cat(paste("\n=================\n NORMALITY in", the_letter, "\n"))

  cat(paste("\n #### Shapiro in", the_letter, "\n"))
  mapply(shapiro.test, the_data)

  ### Multivariate normality
  # To say the data are multivariate normal: 
  # • z-kurtosis < 5 (Bentler, 2006) and the P-value should be ≥ 0.05.
  # • the plot should also form a straight line (Arifin, 2015).

  cat(paste("\n #### Multivariate normality in", the_letter, "\n"))
  mardia(the_data)

  cat(paste("\n=================\n MISSING DATA IN in", the_letter, "\n"))
  ### Missing data

  cat("#### Little's test for MCAR\n")
  #test MCAR
  MCAR <- na.test(the_data)
  mcar_p <- MCAR[["result"]][["pval"]]
  
  cat("\n#### Amount of missing data\n")
  nb_obs <- nrow(the_data)*ncol(the_data)
  na_amount <- sum(is.na(the_data))/nb_obs*100
  cat(paste(as.numeric(na_amount)), "% of data is missing\n\n")
  
  missingTest(na_amount, mcar_p)


### Multiple Imputation for missing values
  cat(paste("\n Multiple Imputation for missing values in", the_letter, "\n"))
  corr_MICE <- make_corrM(the_data, nb_fa = 4)

  cat(paste("\n=================\n FACTORABILITY in", the_letter, "\n"))
  
  ####  Bartlett’s test of sphericity
  # An objective test of the factorability of the correlation matrix is Bartlett’s (1954) test of sphericity, which statistically tests the hypothesis that the correlation matrix contains ones on the diagonal and zeros on the off-diagonals. Hence, that it was generated by random data. This test should produce a statistically significant chi-square value to justify the application of EFA.
  # If the p-value from Bartlett’s Test of Sphericity is lower than our chosen significance level (common choices are 0.10, 0.05, and 0.01), then our dataset is suitable for a data reduction technique.
  cat(paste("\n #### Bartlett in", the_letter, "\n"))
  bartlettTest(corr_MICE, the_data)

  ####  KMO
  # Large sample sizes make the Bartlett test sensitive to even trivial deviations from randomness, so its results should be supplemented with a measure of sampling adequacy. The Kaiser-Meyer-Olkin (KMO; Kaiser, 1974) measure of sampling adequacy is the ratio of correlations and partial correlations that reflects the extent to which correlations are a function of the variance shared across all variables rather than the variance shared by particular pairs of variables. KMO values range from 0.00 to 1.00 and can be computed for the total correlation matrix as well as for each measured variable.
  # - KMO values ≥.70 are desired
  # - KMO values ≤.50 are generally considered unacceptable
  cat(paste("\n #### KMO in", the_letter, "\n"))
  KMOTest(corr_MICE)


  
  cat(paste("\n=================\n RELIABILITY with 4 factors in", the_letter, "\n"))
  ## omega for 4 factors and alpha from data

  om <- omega(corr_MICE, 4, n.obs=nrow(the_data), rotate="Promax")
  print(paste('Alpha:', om$alpha))
  print(paste('G.6:', om$G6))
  print(paste('Omega H asymptotic:', om$omega_h))
  print(paste('Omega Total:', om$omega.tot))
  #summary(om)


  ### Number of factors to retain
  # Measurement specialists have conducted simulation studies and concluded that parallel analysis and MAP are the most accurate empirical estimates of the number of factors to retain and that scree is a useful subjective adjunct to the empirical estimates. Unfortunately, no method has been found to be correct in all situations, so it is necessary to employ multiple methods and carefully judge each plausible solution to identify the most appropriate factor solution.

  # parallelAnalysis(corr_MICE, observations=nb_obs, "All")
}

select_all_variables <- function (dataset, col_list) {
  dataset %>% select(all_of(unlist(col_list)))
}

```

## Multigroup CFA

https://lavaan.ugent.be/tutorial/groups.html

```{r prepare model and data filters}

stimuli_letters <- list("A","B","C","D","E","F")

#we retrieve our model from EFA conducted on all data

## Full model: all items with loadings > 0.32 and no cross-loadings
full_Readability_factors <- list(
  understand = c('obvious', 'meanOveral', 'confid', 'represent', 'understandEasi', 'understandQuick', 'meanElem'),
  layout = c('crowd', 'messi', 'distract', 'organiz'),
  dataRead = c('find', 'identifi', 'valu', 'inform', 'readabl'),
  dataFeat = c('visibl', 'see')
)

# Create a vector of column names to keep for the model with all included items
full_columns_to_keep <- unlist(full_Readability_factors)

this_model <- "Model_1"
## best items selected on factor loadings only
Readability_factors <- list(
  understand = list("obvious","meanOveral","confid"),
  layout = list("crowd", "messi","distract"),
  dataRead = list("find","identifi","valu"),
  dataFeat = list("visibl","see")
)

# Create a vector of column names to keep for the model with selected items
columns_to_keep <- unlist(Readability_factors)

# Function to convert a factor list to the required string format
factor_to_string <- function(factor_name, factor_list) {
  paste(factor_name, " =~ ", paste(factor_list, collapse = " + "))
}

```
## Load data

```{r load data}

my_data <- read.csv(paste("data/ratings-stimulus.csv"), encoding="UTF-8")
grp_data <- subset(my_data, select = -c(seed))
data <- subset(grp_data, select = -c(stimulus))

# and make directories if needed

if (!dir.exists("generatedData-CFA/correlations/")) {
  dir.create("generatedData-CFA/correlations/", recursive = TRUE)
}
if (!dir.exists("generatedData-CFA/reliability/to aggregate/")) {
  dir.create("generatedData-CFA/reliability/to aggregate/", recursive = TRUE)
}

if (!dir.exists("generatedPlots-CFA/correlations/")) {
  dir.create("generatedPlots-CFA/correlations/", recursive = TRUE)
}

```

## Descriptive statistics - Check minimum/maximum values per item, and screen for missing values
```{r data overview}

cat("\n\n==========================================\n
    ==========================================\n
    ==========================================\n")
cat("\n\n BEFORE ITEM REDUCTION\n\n")
cat("==========================================\n
    ==========================================\n
    ==========================================\n\n")

describe(data)
response.frequencies(data)
nb_obs <- nrow(data)
cat("\n Sample size:")
print(nb_obs)

#corr Matrix with all possible items
data_all_factors <- data %>% select(all_of(full_columns_to_keep))
corrM <- make_corrM(data_all_factors, nb_fa = 4)

num_items <- ncol(data_all_factors)
## as csv
write.csv(corrM,file="generatedData-CFA/correlations/factors-all_items-corr.csv")
# as pdf
this_file <- "generatedPlots-CFA/correlations/factors-all_items-corr.pdf"
pdf(this_file, height=15, width=15)
plot <- corrplot(corrM,
        # type = 'lower', #only lower half
        tl.srt = 30, #rotate text labels
        method="color",
        col = COL2('RdYlBu'),
        order="original",
        addrect = 4,
        tl.col="black", tl.cex = 1.3, # Text label color and size
        addCoef.col = "white", number.cex = 1, # Add coefficient of correlation and size
        )
dev.off()

```

```{r multigroup full model CFA}

## Create the model from full_Readability_factors
# Apply the function to each element of the list
full_formatted_strings <- sapply(names(full_Readability_factors), function(factor_name) {
  factor_to_string(factor_name, full_Readability_factors[[factor_name]])
})

# Combine the strings into a single string
full_model_string <- paste(full_formatted_strings, collapse = '\n')

# Print the final string
cat(full_model_string)

cat("\n\n==========================================")
cat("\n==========================================\n")
cat("\n Model fitting for the full factors (BEFORE item reduction) \n")
cat("==========================================\n")
cat("==========================================\n\n")
  Full_fit <- cfa(full_model_string,
           data = grp_data, 
           group = "stimulus",
           missing = "fiml")

# get information per group
summary(Full_fit)
```

### Model fit indices

```{r full model fit indices, echo=FALSE}
#summary(fit, standardized=TRUE, fit.measures=TRUE)
cat("\n==============\n")
cat("Fit indices")
cat("\n==============\n")

# print(fitMeasures(Full_fit, c("chisq", "df", "pvalue", "cfi", "rmsea"), output = "text"), add.h0 = TRUE)

# Full_fit_Indices <- fitMeasures(Full_fit, c("rmsea","rmsea.pvalue","rmsea.ci.lower","rmsea.ci.upper","rmsea.ci.level","srmr","cfi","chisq","df","pvalue"))

Full_fit_Indices <- fitMeasures(Full_fit, output = "matrix")
test_model <- "Full model"
colnames(Full_fit_Indices)[1] <- c(test_model)
write.csv(Full_fit_Indices,file=("generatedData-CFA/fit_indices-full_model.csv"))

```

### GOAL

In their reference work on cutoff criteria for fit indices, Hu and Bentler (1999) recommend a combination of two criteria to retain a model: fit indices such as TLI or CFI should be higher than .95, and SRMR should be lower than 0.9. They add that a combinational cutoff criterion of RMSEA at .06 and SRMR at .09 is possible but less desirable because it tends to reject models that are, in fact, good fits.
https://doi.org/10.1080/10705519909540118


#### SRMR (Standardized Root Mean Square Residual)
> srmr

SRMR is particularly useful for evaluating the discrepancy between the observed and predicted covariance matrices.

SRMR is computed by taking the square root of the mean squared differences between corresponding elements in these two matrices.

The SRMR is then standardized to be independent of the scale of the observed variables. This makes it more comparable across different datasets and models.

A value less than . 08 is generally considered a good fit (Hu & Bentler, 1999)

#### RMSEA (Root Mean Square Error of Approximation)
> rmsea, rmsea.pvalue, rmsea.ci.lowe,r rmsea.ci.upper, rmsea.ci.level

RMSEA is a measure of how well the model's implied relationships between variables match the observed relationships in the data. It measures the average magnitude of the differences between predicted and observed values. In the context of CFA, it looks at how well the model-predicted covariances or correlations match the actual observed covariances or correlations among the variables.

RMSEA values less than 0.05 are good, values between 0.05 and 0.08 are acceptable, values between 0.08 and 0.1 are marginal, and values greater than 0.1 are poor. https://psycnet.apa.org/doi/10.1037/1082-989X.4.3.272
Hu and Bentler (1999) however recommend CFI or TLI + SRMR as validation criterion.

#### CFI (Comparative Fit Index)
> cfi

CFI is calculated by comparing the fit of the hypothesized model to a null model that assumes complete independence among the observed variables. The index ranges from 0 to 1, with higher values indicating better fit. A common rule of thumb is that CFI values above 0.90 or 0.95 are indicative of an acceptable or good fit, respectively.

#### Chi-square
> chisq, df, pvalue

It is calculated by comparing the observed sample covariance matrix with the covariance matrix predicted by the specified model. The formula involves summing the squared differences between the observed and expected (model-implied) covariances, adjusted for the sample size.

The chi-square test requires specifying the degrees of freedom, which depend on the complexity of the model. A small chi-square value relative to its degrees of freedom suggests that the model fits the data well. Conversely, a large chi-square value may indicate a poor fit between the model and the data.

1. Significance Test: The chi-square statistic is associated with a p-value (pvalue) that tests the null hypothesis. If the p-value is small (typically below a chosen significance level, such as 0.05), it suggests that there is evidence to reject the null hypothesis, indicating that the model does not fit the data well. In your provided data, the p-value associated with chisq is 0.000, suggesting that the model significantly deviates from the observed data.

2. Model Complexity: The chi-square statistic is sensitive to model complexity, meaning that as the model becomes more complex (e.g., with more estimated parameters), the chi-square tends to increase. Therefore, larger chi-square values may be expected for more complex models.

3. Sample Size Influence: The chi-square test can be influenced by sample size. In large samples, even small discrepancies between the model and the data can lead to a significant chi-square. Researchers often consider this and use additional fit indices, such as the Comparative Fit Index (CFI), Root Mean Square Error of Approximation (RMSEA), and Standardized Root Mean Square Residual (SRMR), to assess model fit.


```{r measurement invariance full model}

cat("\n Measurement invariance\n")
  
# configural invariance
fit1 <- Full_fit

# weak invariance
fit2 <- cfa(full_model_string, data = grp_data, missing = "fiml", group = "stimulus",
            group.equal = "loadings")

# strong invariance
fit3 <- cfa(full_model_string, data = grp_data, missing = "fiml", group = "stimulus",
            group.equal = c("intercepts", "loadings"))

# model comparison tests
lavTestLRT(fit1, fit2, fit3)

```

# ==================== #
# SECLECTED ITEMS ONLY #
# ==================== #

# Reliability on aggregated data for factors with selected items
```{r reliability of each sub-scale (each factor)}

cat("\n\n==========================================\n")
cat("==========================================\n")
cat("==========================================\n")
cat("\n \n AFTER ITEM REDUCTION: MODEL 1 \n \n")
cat("==========================================\n")
cat("==========================================\n")
cat("==========================================\n\n")


#corr Matrix with selected items
data_reduced_factors <- data %>% select(all_of(columns_to_keep))

corrM <- make_corrM(data_reduced_factors, nb_fa = 4)

num_items <- ncol(data_all_factors)
## as csv
write.csv(corrM,file=paste("generatedData-CFA/correlations/factor-", this_model, "-selected_items-corr.csv", sep =""))
# as pdf
this_file <- paste("generatedPlots-CFA/correlations/factor-", this_model,"-selected_items-corr.pdf", sep="")
pdf(this_file, height=15, width=15)
plot <- corrplot(corrM,
        # type = 'lower', #only lower half
        tl.srt = 30, #rotate text labels
        method="color",
        col = COL2('RdYlBu'),
        order="original",
        addrect = 4,
        tl.col="black", tl.cex = 1.3, # Text label color and size
        addCoef.col = "white", number.cex = 1, # Add coefficient of correlation and size
        )
dev.off()

## Create the model from list Readability_factors
# Apply the function to each element of the list
formatted_strings <- sapply(names(Readability_factors), function(factor_name) {
  factor_to_string(factor_name, Readability_factors[[factor_name]])
})

# Combine the strings into a single string
model_string <- paste(formatted_strings, collapse = '\n')

# Print the final string
cat(model_string)


for (factor in names(Readability_factors)){
  cat(paste("\n ======================\n Individual factor reliability on selected items for", this_model, "-", factor, "\n (omega and alpha) \n"))
  factor_data <- data %>% select(all_of(unlist(Readability_factors[[factor]])))
  
  # reliability
  
  # omega
  nb_obs <- nrow(factor_data)
  corrM <- make_corrM(factor_data, nb_fa = 1)
  om <- omega(corrM, n.obs = nb_obs, 1) #https://personality-project.org/r/psych/HowTo/omega.pdf
  summary(om)

  # alpha
  alpha_values <- psych::alpha(factor_data)
  cat("Alpha raw:")
  print(alpha_values$total$raw_alpha)
  print(alpha_values$total)

  # output df
  factor_alpha_df <- alpha_values$total
  factor_alpha_df$omega_tot <- om$omega.tot
  write.csv(factor_alpha_df,file=paste("generatedData-CFA/reliability/to aggregate/",factor,"-",this_model,"-reliability.csv"))

  # factor-wise reliability for each stimulus
  for (letter in stimuli_letters){
    the_data <- filter(grp_data, stimulus == letter)
    stimulus_factor_data <- the_data %>% select(all_of(unlist(Readability_factors[[factor]])))
    
    # omega
    nb_obs <- nrow(stimulus_factor_data)
    corrM <- make_corrM(stimulus_factor_data, nb_fa = 1)
    om <- omega(corrM, n.obs = nb_obs, 1)
    
    # alpha
    alpha_values <- psych::alpha(stimulus_factor_data)
    
    # output df
    factor_stimulus_alpha_df <- alpha_values$total
    factor_alpha_df$omega_tot <- om$omega.tot
    write.csv(factor_stimulus_alpha_df,file=paste("generatedData-CFA/reliability/to aggregate/",factor,"-",this_model,"-",letter,"-reliability.csv", sep =""))
  }
}

```
#We already ran checks on overall data during EFA. Now we also check individual subgroups
```{r individual group data normality and EFA appropriateness checks}

for (letter in stimuli_letters){
  the_data <- filter(grp_data, stimulus == letter)
  # Subset the columns in the_data
  subset_data <- select_all_variables(the_data, columns_to_keep)
  stimulus_data_test(subset_data, letter)
}

```


```{r run fit analysis}

cat("\n\n==========================================")
cat("\n==========================================\n")
cat("\n Model fitting for the reduced factors (AFTER item reduction) \n")
cat("==========================================\n")
cat("==========================================\n\n")

cat("\n Model fitting\n")
  this_fit <- cfa(model_string,
           data = grp_data, 
           group = "stimulus",
           missing = "fiml")

# get information per group
summary(this_fit)
  
```

### Model fit indices

```{r model fit indices}
#summary(fit, standardized=TRUE, fit.measures=TRUE)

cat("\n==============\n")
cat("Reduced model fit indices")
cat("\n==============\n")
print(fitMeasures(this_fit, c("chisq", "df", "pvalue", "cfi", "rmsea"),
                  output = "text"), add.h0 = TRUE)

fit_Indices <- fitMeasures(this_fit, output = "matrix")
colnames(fit_Indices)[1] <- c(this_model)
write.csv(fit_Indices,file=(paste("generatedData-CFA/fit_indices-", this_model, ".csv", sep="")))


```

### Measurement invariance testing

https://lavaan.ugent.be/tutorial/groups.html#measurement-invariance-testing
NB: Intercepts represent the average score or level of the latent construct when all other variables (including other latent constructs and observed variables) are held constant.

Pr(>Chisq) tells us if the less constrained model fits the data significantly better (e.g., that fit1 fits the data better compared to fit2)

fit1 = basic model, per group analysis
fit2 = model with constraint on factor loadings (equal loadings among groups)
fit3 = model with constraint on both loadings and intercepts


```{r measurement invariance}

cat("\n Measurement invariance\n")
  
# configural invariance
fit1 <- this_fit

# weak invariance
fit2 <- cfa(model_string, data = grp_data, missing = "fiml", group = "stimulus",
            group.equal = "loadings")

# strong invariance
fit3 <- cfa(model_string, data = grp_data, missing = "fiml", group = "stimulus",
            group.equal = c("intercepts", "loadings"))

# model comparison tests
lavTestLRT(fit1, fit2, fit3)

```
