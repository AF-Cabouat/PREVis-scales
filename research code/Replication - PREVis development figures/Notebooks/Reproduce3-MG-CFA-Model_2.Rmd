---
title: "Reliability and CFA Multi-levels analysis - Model_2"
author: "Anne-Flore Cabouat"
date: "2024-01-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Get the current R Markdown notebook file path and the grandparent folder
get_current_rmd_path <- function() {
  if (requireNamespace("rstudioapi", quietly = TRUE) && rstudioapi::isAvailable()) {
    # When running interactively in RStudio
    return(rstudioapi::getActiveDocumentContext()$path)
  } else if (requireNamespace("knitr", quietly = TRUE)) {
    # When knitting the document
    return(knitr::current_input(dir = TRUE)) 
  } else {
    stop("Cannot determine the current R Markdown file path.")
  }
}

current_rmd_path <- get_current_rmd_path()
my_dir <- dirname(dirname(current_rmd_path))


#### OPTIONAL: manual setting of directory
#my_dir <- "your_data_directory" #eg: "C:/Absolute_path_to_git/PREVis-scales/research code/Replication - PREVis development figures"

# We create the output folders if they don't exist
if (!dir.exists(file.path(my_dir, "Data_Analysis"))) {
  dir.create(file.path(my_dir, "Data_Analysis"), recursive = TRUE)
}

# We set our working directory to the Data_Analysis folder so that all outputs are saved there
my_wd <- file.path(my_dir, "Data_Analysis")
setwd(my_wd)
knitr::opts_knit$set(root.dir = file.path("..", "Data_Analysis"))

# Print the current working directory to verify
print(getwd())

```

## Librairies

```{r librairies}

load_and_install_libraries <- function(libraries) {
  for (lib in libraries) {
    if (!require(lib, character.only = TRUE)) {
      install.packages(lib)
      library(lib, character.only = TRUE)
    }
  }
}

required_libraries <- c("psych", "misty", "mice", "mifa", "corrplot", "knitr", "lavaan", "dplyr", "RColorBrewer")
load_and_install_libraries(required_libraries)
```

## Functions

```{r functions}

make_corrM <- function(d, order = NULL, nb_fa = 4){
  # Multiple imputation using Fully Conditional Specification (FCS) implemented by the MICE algorithm as described in Van Buuren and Groothuis-Oudshoorn (2011) doi:10.18637/jss.v045.i03
  #run MICE and generate covariance matrix with with mifa package
  mi <- invisible(mifa(
  data      = d, 
  n_pc      = nb_fa, #factors estimates
  ci        = "fieller", #confidence interval method
  print     = FALSE,
  m = 5 #number of MICE iterations
  ))
  
  corr_MICE <- cov2cor(mi$cov_combined)
  
  if (length(order)>0){
    corrM_reorder <- corr_MICE[order, order]
    corr_MICE <- corrM_reorder
  }
  
  # corrM <- corFiml(d)
  return(corr_MICE)
}

# Applicability of EFA

## Bartlett test of sphericity - from TH
bartlettTest <- function(corrMatrix,data){
  bart <- cortest.bartlett(corrMatrix, n = nrow(data))
  if(bart[2]>0.05) cat("WARNING the p value is above 0.05") else cat("The p value is below 0.05. We are good to continue.")
  cat("\n\n")
  print(bart)
  return(bart)
}

##KMO to complement Bartlett - from TH
KMOTest <- function(corrMatrix){
  kmo <- KMO(corrMatrix)
  cat(paste("The overall measure of sampling adequacy is: ",kmo[1]))
  cat("\n\n")
  if(kmo[1]<.7) cat("WARNING the sampling adequacy has dropped below 0.7") else cat("The sampling adequacy is above 0.7. We are generally good.")
  cat("\n\n")
  return(kmo)
}

# Missing values

missingTest <- function(na_percent, mcar_pvalue){
  if (na_percent<5 & mcar_pvalue<=.05){
      cat("Data is MCAR and less than 5% missing data, which is negligible. We are good to continue.")
  } else if (na_percent<5 & mcar_pvalue>.05){
      cat("Data is NOT MCAR but less than 5% missing data, which is negligible. We are good to continue.")
  } else if(na_percent>=5 & mcar_pvalue>.05) {
      cat("WARNING missing data is not negligible (> 5%) AND missing data is NOT MCAR. Further investigation is needed.")
    }else if (na_percent>=5 & na_percent<40 & mcar_pvalue<=.05){
      cat("Data is MCAR but more than 5% missing data. Use imputation / likelihood methods.")
    }  else {
      cat("40% or more missing data and / or something is wrong.")
    }}


# Parallel analysis for each group

parallelAnalysis <- function(corrMatrix, observations = nrow(the_data), file_suffix){
  pdf(paste(paste("generatedPlots-EFA/ScreePlot-",file_suffix,sep=""),'.pdf'), width=8, height=4)
  parallel <- fa.parallel(corrMatrix, n.obs=observations, fa="fa", n.iter=100, main="Scree plots with parallel analysis", error.bars = TRUE)
  print(parallel)
  dev.off() 
  cat("\n\n")
}

stimulus_data_test <- function(the_data, the_letter){
  ## Here wwe run test on the data
  ### Shapiro Wilk’s test (P-values < 0.05 indicate not normal)
  cat(paste("\n=================\n NORMALITY in", the_letter, "\n"))

  cat(paste("\n #### Shapiro in", the_letter, "\n"))
  mapply(shapiro.test, the_data)

  ### Multivariate normality
  # To say the data are multivariate normal: 
  # • z-kurtosis < 5 (Bentler, 2006) and the P-value should be ≥ 0.05.
  # • the plot should also form a straight line (Arifin, 2015).

  cat(paste("\n #### Multivariate normality in", the_letter, "\n"))
  mardia(the_data)

  cat(paste("\n=================\n MISSING DATA IN in", the_letter, "\n"))
  ### Missing data

  cat("#### Little's test for MCAR\n")
  #test MCAR
  MCAR <- na.test(the_data)
  mcar_p <- MCAR[["result"]][["pval"]]
  
  cat("\n#### Amount of missing data\n")
  nb_obs <- nrow(the_data)*ncol(the_data)
  na_amount <- sum(is.na(the_data))/nb_obs*100
  cat(paste(as.numeric(na_amount)), "% of data is missing\n\n")
  
  missingTest(na_amount, mcar_p)


### Multiple Imputation for missing values
  cat(paste("\n Multiple Imputation for missing values in", the_letter, "\n"))
  corr_MICE <- make_corrM(the_data, nb_fa = 4)

  cat(paste("\n=================\n FACTORABILITY in", the_letter, "\n"))
  
  ####  Bartlett’s test of sphericity
  # An objective test of the factorability of the correlation matrix is Bartlett’s (1954) test of sphericity, which statistically tests the hypothesis that the correlation matrix contains ones on the diagonal and zeros on the off-diagonals. Hence, that it was generated by random data. This test should produce a statistically significant chi-square value to justify the application of EFA.
  # If the p-value from Bartlett’s Test of Sphericity is lower than our chosen significance level (common choices are 0.10, 0.05, and 0.01), then our dataset is suitable for a data reduction technique.
  cat(paste("\n #### Bartlett in", the_letter, "\n"))
  bartlettTest(corr_MICE, the_data)

  ####  KMO
  # Large sample sizes make the Bartlett test sensitive to even trivial deviations from randomness, so its results should be supplemented with a measure of sampling adequacy. The Kaiser-Meyer-Olkin (KMO; Kaiser, 1974) measure of sampling adequacy is the ratio of correlations and partial correlations that reflects the extent to which correlations are a function of the variance shared across all variables rather than the variance shared by particular pairs of variables. KMO values range from 0.00 to 1.00 and can be computed for the total correlation matrix as well as for each measured variable.
  # - KMO values ≥.70 are desired
  # - KMO values ≤.50 are generally considered unacceptable
  cat(paste("\n #### KMO in", the_letter, "\n"))
  KMOTest(corr_MICE)


  
  cat(paste("\n=================\n RELIABILITY with 4 factors in", the_letter, "\n"))
  ## omega for 4 factors and alpha from data

  om <- omega(corr_MICE, 4, n.obs=nrow(the_data), rotate="Promax")
  print(paste('Alpha:', om$alpha))
  print(paste('G.6:', om$G6))
  print(paste('Omega H asymptotic:', om$omega_h))
  print(paste('Omega Total:', om$omega.tot))
  #summary(om)


  ### Number of factors to retain
  # Measurement specialists have conducted simulation studies and concluded that parallel analysis and MAP are the most accurate empirical estimates of the number of factors to retain and that scree is a useful subjective adjunct to the empirical estimates. Unfortunately, no method has been found to be correct in all situations, so it is necessary to employ multiple methods and carefully judge each plausible solution to identify the most appropriate factor solution.

  # parallelAnalysis(corr_MICE, observations=nb_obs, "All")
}

select_all_variables <- function (dataset, col_list) {
  dataset %>% select(all_of(unlist(col_list)))
}

```

## Multigroup CFA

https://lavaan.ugent.be/tutorial/groups.html

```{r prepare model and data filters}

stimuli_letters <- list("A","B","C","D","E","F")

this_model <- "Model_2"
## best items selected on reliability measures only
Readability_factors <- list(
  understand = list("understandEasi","represent","understandQuick"),
  layout = list("messi", "organiz","crowd"),
  dataRead = list("inform","identifi","find"),
  dataFeat = list("visibl","see")
)

# Create a vector of column names to keep for the model with selected items
columns_to_keep <- unlist(Readability_factors)

# Function to convert a factor list to the required string format
factor_to_string <- function(factor_name, factor_list) {
  paste(factor_name, " =~ ", paste(factor_list, collapse = " + "))
}

```
## Load data

```{r load data}

setwd(my_wd)

if (!dir.exists("generatedData-CFA/")) {
  stop("You need to run reliability and Model_1 fit analyses first.")
}

data_dir <- file.path(my_dir, "Data")
file_path <- file.path(data_dir, "ratings-stimulus.csv")
my_data <- read.csv(file_path, encoding="UTF-8")
grp_data <- subset(my_data, select = -c(seed))
data <- subset(grp_data, select = -c(stimulus))

```


# ==================== #
# SECLECTED ITEMS ONLY #
# ==================== #


#We already ran checks on overall data during EFA. Now we also check individual subgroups
```{r individual group data normality and EFA appropriateness checks}

setwd(my_wd)

for (letter in stimuli_letters){
  the_data <- filter(grp_data, stimulus == letter)
  # Subset the columns in the_data
  subset_data <- select_all_variables(the_data, columns_to_keep)
  stimulus_data_test(subset_data, letter)
}

```


```{r run fit analysis}

cat("\n\n==========================================")
cat("\n==========================================\n")
cat("\n Model fitting for the reduced factors (AFTER item reduction) \n")
cat("==========================================\n")
cat("==========================================\n\n")

cat("\n Model fitting\n")
  this_fit <- cfa(model_string,
           data = grp_data, 
           group = "stimulus",
           missing = "fiml")

# get information per group
summary(this_fit)
  
```

### Model fit indices

```{r model fit indices}

#summary(fit, standardized=TRUE, fit.measures=TRUE)

cat("\n==============\n")
cat("Reduced model fit indices")
cat("\n==============\n")
print(fitMeasures(this_fit, c("chisq", "df", "pvalue", "cfi", "rmsea"),
                  output = "text"), add.h0 = TRUE)

fit_Indices <- fitMeasures(this_fit, output = "matrix")
colnames(fit_Indices)[1] <- c(this_model)
write.csv(fit_Indices,file=(paste("generatedData-CFA/fit_indices-", this_model, ".csv", sep="")))


```

### Measurement invariance testing

https://lavaan.ugent.be/tutorial/groups.html#measurement-invariance-testing
NB: Intercepts represent the average score or level of the latent construct when all other variables (including other latent constructs and observed variables) are held constant.

Pr(>Chisq) tells us if the less constrained model fits the data significantly better (e.g., that fit1 fits the data better compared to fit2)

fit1 = basic model, per group analysis
fit2 = model with constraint on factor loadings (equal loadings among groups)
fit3 = model with constraint on both loadings and intercepts


```{r measurement invariance}

cat("\n Measurement invariance\n")
  
# configural invariance
fit1 <- this_fit

# weak invariance
fit2 <- cfa(model_string, data = grp_data, missing = "fiml", group = "stimulus",
            group.equal = "loadings")

# strong invariance
fit3 <- cfa(model_string, data = grp_data, missing = "fiml", group = "stimulus",
            group.equal = c("intercepts", "loadings"))

# model comparison tests
lavTestLRT(fit1, fit2, fit3)

```
