---
title: "IRT graded response model"
author: "Anne-Flore Cabouat"
date: "2024-03-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#set wordking directory
# my_dir <- "your_data_directory" #eg: "absolute_path_to/Supplementary-material-folder/2 - Scale development/"
my_dir <- "/Users/anne-flore/git/readability-rebased/Supplementary-material-folder/2 - Scale development/"
knitr::opts_knit$set(root.dir = paste(my_dir, "Results/Data_Analysis", sep=""))
```

```{r libraies}
library(tidyverse)
library(mirt)
library(lavaan)
library(directlabels)
library(gridExtra)
library(kableExtra)
# library(RColorBrewer)
```

```{r functions}

# Check compliance with general recommendations for IRT
fit_eval <- function(fit_statistics_object) {
  if (fit_statistics_object$RMSEA > 0.06) {
    print(paste("Warning: IRT RMSEA suggests insufficient fit (Value: ", round(fit_statistics_object$RMSEA, 4), "- Maximum: 0.06). Consider model improvement."))
  }
  if (fit_statistics_object$SRMSR > 0.09) {
    print(paste("Warning: IRT SRMSR suggests insufficient fit (Value: ", round(fit_statistics_object$SRMSR, 4), "- Maximum: 0.07). Consider model improvement."))
  }
  if (fit_statistics_object$TLI < 0.90) {
    print(paste("Warning: IRT TLI suggests insufficient fit (Value: ", round(fit_statistics_object$TLI, 4), "- Minimum: 0.90). Consider model improvement."))
  }
  if (fit_statistics_object$CFI < 0.90) {
    print(paste("Warning: IRT CFI suggests insufficient fit (Value: ", round(fit_statistics_object$CFI, 4), "- Minimum: 0.90). Consider model improvement."))
  }
}

correlation_CTT_IRT_eval <- function(cor_test){
  # Set a threshold for what you consider a high correlation
  correlation_threshold <- 0.9
  
  # Display a message based on the correlation value
  if (cor_test$estimate > correlation_threshold) {
    cat("The correlation is", round(cor_test$estimate, 4), "(positive outcome).",
        "Both models provide similar information about the latent trait.",
        "The threshold for a high correlation is", correlation_threshold, ".")
  } else {
    cat("The correlation is", round(cor_test$estimate, 4), ".",
        "While there is a significant correlation, consider exploring reasons for the difference.",
        "The threshold for a high correlation is", correlation_threshold, ".")
  }
}

single_factor_IRT_analysis <- function(d, stimulus, context, cfa_model_list, factor_name){
  
  cat(paste('\n=========\n Runnning IRT analysis for ', stimulus, ' - Factor: ', factor_name, '\n==========\n', sep=''))
  
  cat("Check missing data")
  
  nb_obs <- nrow(d)
  nb_answers <- nb_obs*29 #29 rating items in our questionnaire
  
  # Check for missing data
  if (any(is.na(d))) {
    cat("Warning: Missing data detected. Removing cases with missing values.\n")
    na_amount <- sum(is.na(d))/nb_answers*100
    cat(paste(as.numeric(na_amount)), "% of data is missing\n\n")
    # Remove cases with missing values
    d <- na.omit(d)
  }
  
  cat("\n\n Fit a mirt model")
  
  fitGraded <- mirt(d, 1, itemtype = "graded") #1 factor only, no group
  
  # Using coef() extracts the maximum-likelihood parameters and item facilities (i.e., average number of correct responses)
  coef(fitGraded)
  
  # Compute M2* statistic and associated fit indices
  
  # Attempt with M2*
  fit_statistics <- NULL  # Initialize to NULL or any default value
  tryCatch({
    fit_statistics <- M2(fitGraded, type = "M2*", na.rm = TRUE)
  }, error = function(e1) {
    # If there's an error, attempt with C2
    tryCatch({
      fit_statistics <- M2(fitGraded, type = "C2", na.rm = TRUE)
    }, error = function(e2) {
      # If there's an error with both, print messages
      cat("Error with both M2* and C2:", conditionMessage(e1), "\n", conditionMessage(e2), "\n")
    })
  })
  
  # Print and evaluate fit statistics if computed successfully
  if (!is.null(fit_statistics)) {
    print('printing fit statistics')
    print(fit_statistics)
    kable(fit_statistics, format = "html") %>% kable_styling()
    fit_eval(fit_statistics)
  }
  
  # fit_statistics <- M2(fitGraded, type = "C2", na.rm = TRUE) #type = "M2*"
  # print('printing fit statistics')
  # print(fit_statistics)
  # kable(fit_statistics, format = "html") %>% kable_styling()
  # 
  # fit_eval(fit_statistics)
  
  ### Explanation of fit statistics:
  # (M2*): Measure of model-implied vs. observed covariance. Lower is better.
  # df: Degrees of freedom associated with the model fit.
  # p: P-value for the M2 statistic. Lower values suggest better fit.
  # RMSEA: Root Mean Square Error of Approximation. Closer to 0 is better (< 0.08 is acceptable).
  # SRMSR: Standardized Root Mean Square Residual. Lower is better (< 0.08 is acceptable).
  # TLI: Tucker-Lewis Index. Closer to 1 is better (> 0.90 is acceptable).
  # CFI: Comparative Fit Index. Closer to 1 is better (> 0.90 is acceptable).
  
  if (n_factors == 1) {
    cat("\n\n Check against CTT EFA")
    
    fitCTT <- cfa(cfa_model_list, data = d)
    
    standardizedsolution(fitCTT) %>%
    filter(op == "=~") %>% select(rhs, F1 = est.std)
    
    
    ## See if CTT and IRT converge towards similar interpretations
    # A high positive correlation supports the idea that IRT and CTT models both capture similar aspects of the construct being measured, event though they have different underlying assumptions and methodologies.
    
    #cor.test(predict(fitCTT), fscores(fitGraded))
    cor_test_result <- cor.test(predict(fitCTT), fscores(fitGraded))
    print(cor_test_result)
    correlation_CTT_IRT_eval(cor_test_result)
}
  
  cat("\n\nProceed with IRT analysis of items\n")
  params <- coef(fitGraded, IRTpars = TRUE, simplify = TRUE)
  kable(round(params$items, 2), format = "html") %>%
    kable_styling()
  
  csv_filename <- paste('generatedData-IRT/', context, stimulus, '-', factor_name, '-Items-Information.csv', sep = '')
  write.csv(round(params$items, 2), csv_filename, row.names = TRUE)
  cat(paste(csv_filename, 'was exported \n'))
  
  
  # Print each item's trace individually
  
  for (item_name in colnames(d)) {
    pdf(paste('generatedPlots-IRT/', context, 'individual items plots/', stimulus, '-', factor_name,'-',item_name,'-trace.pdf', sep=''), height = 6, width = 6)
    item_plot <- itemplot(fitGraded,
                          item_name,
                          type = 'trace',
                          auto.key=list(points=F, lines=T, columns=1, space = 'top', cex = .8),
                          theta_lim = c(-3, 3),
                          # par.settings = my_colors,
                          drape = TRUE,
                          colorkey = TRUE,
                          main= paste("Item Characteristic Curve for", item_name)
                          )
    direct.label(item_plot, 'top.points')
    print(item_plot)
    dev.off()
  }
  
  # Count the number of items
  num_items <- ncol(d)
  
  # print(num_items)
  
  if (num_items < 3){
    pdf_height <- 9
  } else {
    pdf_height <- 15
  }
  pdf_width <- 15
  # print(pdf_height)
  pdf(paste('generatedPlots-IRT/', context, stimulus, '-', factor_name,'-Items_traces.pdf', sep=''), height = pdf_height, width = pdf_width)
  plt <- plot(fitGraded, type='trace',
           facet_items=T, 
           as.table = TRUE,
           auto.key=list(points=F, lines=T, columns=1, space = 'top', cex = .8),
           theta_lim = c(-3, 3),
           # par.settings = my_colors,
           drape = TRUE,
           colorkey = TRUE, 
           main = "")
  print(plt) #it is necessary to print because we are in a function
  
  dev.off()
  
  pdf(paste('generatedPlots-IRT/', context, stimulus, '-', factor_name,'-All_Items-Information.pdf', sep=''), height=15, width=15)

  plt_all <- plot(fitGraded, type='trace',
       facet_items=FALSE, 
       as.table = TRUE,
       auto.key=list(points=F, lines=T, columns=1, space = 'top', cex = .8),
       theta_lim = c(-3, 3),
       # par.settings = my_colors,
       drape = TRUE,
       colorkey = TRUE, 
       main = "")
  
  print(plt_all) #it is necessary to print because we are in a function
  dev.off()
  
}

generate_model_list <- function(factor_name, column_names) {
  formula <- paste(factor_name, " =~ ", paste(column_names, collapse = " + "), sep='')
  return(formula)
}

```

```{r prepare}

# Create folders if they don't exist

if (!dir.exists("generatedData-IRT/")) {
  dir.create("generatedData-IRT/", recursive = TRUE)
}

if (!dir.exists("generatedData-IRT/")) {
  dir.create("generatedData-IRT/", recursive = TRUE)
}

path <- "data/"

stimuli_letters <- list("A","B","C","D","E","F","Agg")
stimuli_numbers <- list(1,2,3,4,5,6,7)

n_factors = 1

```

```{r run IRT on all data with 1 factor, include=FALSE}
# 
# if (!dir.exists("generatedPlots-IRT/individual items plots/")) {
#   dir.create("generatedPlots-IRT/individual items plots/", recursive = TRUE)
# }
# 
# full_model_list = "readability =~ answer + clearData + clearRepresent + complex + confid + confus + crowd + deciph + distinguish + distract + effect + find + identifi + inform + lost + meanElem + meanOveral + messi + obvious + organiz + read + readabl + represent + see + simpl + understandEasi + understandQuick + valu + visibl"
# 
# my_data <- read.csv(paste(path,"ratings-stimulus.csv",sep=""), encoding="UTF-8")
# grp_data <- subset(my_data, select = -c(seed))
# d <- subset(grp_data, select = -c(stimulus))
# single_factor_IRT_analysis(d, "Aggregate data", "", full_model_list, "")

```

# Separate IRT for each subscale
!! No example found in the literature for factors with less than 5 items
Valdivia and Dai (2024) recommend 10 items rather than 5.

```{r run separately on each of the 4 factors identified in EFA}
# NOT APPROPRIATE: TOO LITTLE ITEMS PER FACTOR FOR IRT in Layout


if (!dir.exists("generatedPlots-IRT/4-factors solution/individual items plots/")) {
  dir.create("generatedPlots-IRT/4-factors solution/individual items plots/", recursive = TRUE)
}

if (!dir.exists("generatedData-IRT/4-factors solution/")) {
  dir.create("generatedData-IRT/4-factors solution/", recursive = TRUE)
}

full_Readability_factors <- list(
  understand = c('obvious', 'meanOveral', 'confid', 'represent', 'understandEasi', 'understandQuick', 'meanElem'),
  layout = c('crowd', 'messi', 'distract', 'organiz'),
  dataRead = c('find', 'identifi', 'valu', 'inform', 'readabl'),
  dataFeat = c('visibl', 'see')
)

# Create a vector of column names to keep for the model with all included items
full_columns_to_keep <- unlist(full_Readability_factors)

this_context <- "4-factors solution/"

get_factor_name <- function(factor) {
  deparse(substitute(factor))
}

for (num in stimuli_numbers){
  #load data
  file = paste('ratings-',num,'.csv',sep='')
  my_data <- read.csv(paste(path,file,sep=""), encoding="UTF-8")
  
  #retrieve this stimulus' name
  stimulus <- stimuli_letters[[num]]
  
  #for each of the 4 factors identified in EFA (see "EFA + all" file)
  for (factor_name in names(full_Readability_factors)){
    Factor <- full_Readability_factors[[factor_name]]
    print(Factor)
    
    #check that all names are valid
    valid_columns <- intersect(names(my_data), Factor)
    model_list <- generate_model_list("readability", Factor)
    
    if (length(Factor)==length(valid_columns)){
      d <- my_data %>% select(all_of(Factor))

      single_factor_IRT_analysis(d, stimulus, this_context, model_list, factor_name)
    } else {
      print('Some names are wrong in the factor list, check below')
      print(sort(Factor))
      print(sort(valid_columns))
    }
  }
}

```

```{r run one factor with all data}

if (!dir.exists("generatedData-IRT/1-factor solution/")) {
  dir.create("generatedData-IRT/1-factor solution/", recursive = TRUE)
}

if (!dir.exists("generatedPlots-IRT/1-factor solution/individual items plots/")) {
  dir.create("generatedPlots-IRT/1-factor solution/individual items plots/", recursive = TRUE)
}

my_data <- read.csv(paste(path,"ratings-stimulus.csv",sep=""), encoding="UTF-8")
grp_data <- subset(my_data, select = -c(seed))
d <- subset(grp_data, select = -c(stimulus))

this_context <- "1-factor solution/"
this_factor_name = "full_data"
column_names <- colnames(d)
full_model_list <- generate_model_list("full_data", column_names)
single_factor_IRT_analysis(d, stimulus, this_context, full_model_list, this_factor_name)

# for (num in stimuli_numbers){
#   #load data
#   file = paste('ratings-',num,'.csv',sep='')
#   my_data <- read.csv(paste(path,file,sep=""), encoding="UTF-8")
# 
#   #add all cols to the list for comparative cfa
#   column_names <- colnames(my_data)
#   full_model_list <- generate_model_list("full_data", column_names)
# 
#   #retrieve this stimulus' name
#   stimulus <- stimuli_letters[[num]]
# 
#   cat('\nUnidimensional analysis\n')
# 
#   #run the analysis
#   single_factor_IRT_analysis(my_data, stimulus, this_context, full_model_list, this_factor_name)
# }

```

# IRT concepts

*a (Discrimination Parameter):* measures how effectively the item discriminates between respondents with different levels of the latent trait.
Higher values of a indicate stronger discrimination, suggesting that the item is better at distinguishing individuals with varying degrees of the latent trait.

*b (Difficulty Parameters - b1 to b6):* represent the points along the latent trait continuum where a respondent has a 50% probability of endorsing a particular response category or higher.
There are m-1 location parameters where m refers to the number of response categories on the response scale (in this case, 7-1 = 6). Each b parameter corresponds to the difficulty level of endorsing the associated response category. Lower b values indicate that the item is easier, requiring a lower level of the latent trait for endorsement.

The presence of NA in the difficulty parameters indicates that the estimation procedure did not converge or produce reliable estimates for those specific parameters. This can happen when there are not enough data points or the data distribution is such that the model struggles to precisely estimate the thresholds.

It doesn't necessarily mean that zero participants chose an option to the right of the difficulty threshold. Instead, it suggests that the model couldn't confidently estimate the precise difficulty level for those response categories.

*c (Guessing Parameter):* reflects the probability that a respondent with very low ability will answer the item correctly purely by chance, especially for the lowest response category.
Smaller c values are desirable, indicating a lower likelihood of guessing and minimizing the impact of chance guessing.



If you encounter NAs in the difficulty parameters, it's essential to investigate the data distribution, response patterns, and the overall model fit. It might be worth considering simplifications to the model or exploring ways to address potential issues with the data.

Simplification of the model in the context of Item Response Theory (IRT) refers to making adjustments or modifications to the model structure to address issues such as convergence problems, overfitting, or difficulties in estimating parameters. Here are a few ways you might simplify the model:

-*Reducing the Number of Parameters:* If the model has a large number of parameters relative to the amount of available data, it might be overfitting the data. Consider reducing the number of latent traits or item parameters to achieve a better balance.
-*Combining Categories:* If certain response categories have sparse data or are causing estimation issues, you could consider collapsing or combining categories. This reduces the number of parameters and can make the model more robust.
-*Constraint Imposition:* Apply constraints to certain parameters to simplify the model structure. For example, fixing certain thresholds or loadings to specific values can sometimes stabilize the estimation process.
-*Exploratory Factor Analysis (EFA):* In some cases, if the measurement structure is not well-defined, starting with an exploratory factor analysis to identify underlying factors might be a simpler approach.
-*Item Removal:* If certain items consistently cause convergence issues or have limited variability, consider removing them from the analysis.

The goal of simplification is to strike a balance between model complexity and the amount of available data. Overly complex models can lead to overfitting, where the model fits the noise in the data rather than capturing the underlying patterns. Simplifying the model is an iterative process, and it often involves a careful examination of the data, model fit statistics, and the interpretability of the results.



## Code sources

tutorial at https://github.com/ccs-amsterdam/r-course-material/blob/master/tutorials/R_test-theory_3_irt_graded.md
and https://bookdown.org/bean_jerry/using_r_for_social_work_research/item-response-theory.html
