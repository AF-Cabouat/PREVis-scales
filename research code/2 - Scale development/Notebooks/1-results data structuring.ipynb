{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare our variables\n",
    "dir = '../Results'\n",
    "\n",
    "#per stimuli out directory\n",
    "outdir = f'{dir}/Data_Analysis'\n",
    "\n",
    "#create output folders\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "if not os.path.exists(f'{outdir}/others'):\n",
    "    os.makedirs(f'{outdir}/others')\n",
    "\n",
    "if not os.path.exists(f'{outdir}/data'):\n",
    "    os.makedirs(f'{outdir}/data')\n",
    "\n",
    "conditions = [\n",
    "    'A',\n",
    "    'B',\n",
    "    'C',\n",
    "    'D',\n",
    "    'E',\n",
    "    'F'\n",
    "]\n",
    "\n",
    "#different groups of column names\n",
    "Prolific_demographics = [\n",
    "    'Age',\n",
    "    'Gender',\n",
    "    'Highest education level completed',\n",
    "    'Language',\n",
    "    'Fluent languages'\n",
    "]\n",
    "\n",
    "demographics = [\n",
    "    'Age',\n",
    "    'Gender',\n",
    "    'Education',\n",
    "    'English_primary',\n",
    "    'English_fluent',\n",
    "]\n",
    "\n",
    "\n",
    "items = [\n",
    "'answer',\n",
    "'attentionCheck',\n",
    "'clearData',\n",
    "'clearRepresent',\n",
    "'complex',\n",
    "'confid',\n",
    "'confus',\n",
    "'crowd',\n",
    "'deciph',\n",
    "'distinguish',\n",
    "'distract',\n",
    "'effect',\n",
    "'find',\n",
    "'identifi',\n",
    "'inform',\n",
    "'lost',\n",
    "'meanElem',\n",
    "'meanOveral',\n",
    "'messi',\n",
    "'obvious',\n",
    "'organiz',\n",
    "'read',\n",
    "'readabl',\n",
    "'represent',\n",
    "'see',\n",
    "'simpl',\n",
    "'understandEasi',\n",
    "'understandQuick',\n",
    "'valu',\n",
    "'visibl',\n",
    "]\n",
    "\n",
    "#groups of answers (because we downladed the answers' values, not their codes)\n",
    "#how we will code ratings\n",
    "ratings = {\n",
    "    'Strongly disagree':1,\n",
    "    'Disagree':2,\n",
    "    'Slightly disagree':3,\n",
    "    'Neutral':4,\n",
    "    'Slightly agree':5,\n",
    "    'Agree':6,\n",
    "    'Strongly agree':7,\n",
    "    'Other': '' #NA for \"Dont' know\"\n",
    "}\n",
    "\n",
    "#correct answers for the 3 reading tasks, for each stimuli\n",
    "correct_answers = {\n",
    "    'A': {\n",
    "        'ATaskRV':'20.76 Mbps',\n",
    "        'ATaskFE':'Papua New Guinea',\n",
    "        'ATaskTopic':'Broadband downloading speed in Oceania',\n",
    "        'ATaskTopicTryAgain':'Broadband downloading speed in Oceania'\n",
    "    },\n",
    "    'B': {\n",
    "        'BTaskRV':'3.55$',\n",
    "        'BTaskFCT':'Increasing',\n",
    "        'BTaskTopic':'Evolution of fruits prices between January and March',\n",
    "        'BTaskTopicTryAgain':'Evolution of fruits prices between January and March'\n",
    "    },\n",
    "    'C': {\n",
    "        'CTaskFE':'USA',\n",
    "        'CTaskMC':'more from China',\n",
    "        'CTaskTopic':'Distribution of students enrolled in an online program',\n",
    "        'CTaskTopicTryAgain':'Distribution of students enrolled in an online program'\n",
    "    },\n",
    "    'D': {\n",
    "        'DTaskMC':'True',\n",
    "        'DTaskFE':'Housing',\n",
    "        'DTaskTopic':'Average distribution of European households’ spendings',\n",
    "        'DTaskTopicTryAgain':'Average distribution of European households’ spendings'\n",
    "    },\n",
    "    'E': {\n",
    "        'ETaskRV':'False',\n",
    "        'ETaskCl':'3',\n",
    "        'ETaskTopic':'The family tree of a student',\n",
    "        'ETaskTopicTryAgain':'The family tree of a student'\n",
    "    },\n",
    "    'F': {\n",
    "        'FTaskRV':'17,636€',\n",
    "        'FTaskFCT':'Increasing or slightly increasing',\n",
    "        'FTaskTopic':'Evolution of sales profits on office supplies and equipment',\n",
    "        'FTaskTopicTryAgain':'Evolution of sales profits on office supplies and equipment'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_col_names(my_df,\n",
    "                  my_string,\n",
    "                  range_max=30): #the range defines how far in the col name string we will look for the string to match\n",
    "    col_names = []\n",
    "    for col in list(my_df):\n",
    "        if my_string in col[0:range_max+1]:\n",
    "            col_names.append(col)\n",
    "    return col_names\n",
    "\n",
    "\n",
    "def combine_questions_codes(items_list, combine_with, comb_position='after_item', prefix='', suffix=''):\n",
    "    combined_list = []\n",
    "    for comb in combine_with:\n",
    "        for item in items_list:\n",
    "            if comb_position == 'after_item':\n",
    "                combination = prefix+item+comb+suffix\n",
    "                combined_list.append(combination)\n",
    "            elif comb_position == 'before_item':\n",
    "                combination = prefix+comb+item+suffix\n",
    "                combined_list.append(combination)\n",
    "    return combined_list\n",
    "\n",
    "def drop_times(my_df):\n",
    "    len_before = len(list(my_df))\n",
    "    for col_name in list(my_df):\n",
    "        if 'Time' in col_name:\n",
    "            my_df = my_df.drop(col_name, axis=1)\n",
    "    print(f'Dropped {len_before-len(list(my_df))} Question time columns')\n",
    "    return my_df\n",
    "\n",
    "def drop_full_NaN_cols(my_df):\n",
    "    list_before = list(my_df)\n",
    "    my_df = my_df.dropna(axis=1, how='all')\n",
    "    print(f'Dropped {len(list_before)-len(list(my_df))} fully empty columns')\n",
    "    dropped_cols_list = [col for col in list_before if col not in list(my_df)]\n",
    "    print(dropped_cols_list)\n",
    "    return my_df\n",
    "\n",
    "def drop_lines_by_NaN_in_col(my_df, my_col, na_param = \"na\"):\n",
    "    excluded_df = pd.DataFrame(columns = list(my_df))\n",
    "    len_before = len(my_df)\n",
    "    for i, row in my_df.iterrows():\n",
    "        if na_param == \"na\":\n",
    "            if pd.isna(my_df.at[i, my_col]):\n",
    "                excluded_df.loc[i]= row\n",
    "                my_df.drop(index=[i], inplace=True)\n",
    "        elif na_param == \"not_na\":\n",
    "            if not pd.isna(my_df.at[i, my_col]):\n",
    "                excluded_df.loc[i]= row\n",
    "                my_df.drop(index=[i], inplace=True)\n",
    "        else:\n",
    "            print(\"Improper value for na_param: either 'na' or 'not_na' (default='na')\")\n",
    "    print(f'Dropped {len_before-len(my_df)} participants')\n",
    "    return (my_df, excluded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate data by stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anne-Flore\\AppData\\Local\\Temp\\ipykernel_25032\\2294128658.py:1: DtypeWarning: Columns (7,8,17,20,22,30,33,35,37,44,47,49,52,56,62,68,72,74,83,91,93,102,103,108,113,117,119,124,129,132,134,141,145,149,150,156,160,162,167,170,173,177,179,182,186,188,191,195,197,199,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,477,478,479,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,504,505,506,507,508,509,510,511,512,513,514,515) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f'{dir}/results_cleaned.csv', dtype={'seed': object}).set_index('seed')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f'{dir}/results_cleaned.csv', dtype={'seed': object}).set_index('seed')\n",
    "demographics += ['colorDeficiency','colorDeficiency_comment_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check uniqueness of ids\n",
    "all_seeds = list(df.index)\n",
    "if len(set(all_seeds)) < len(all_seeds):\n",
    "    print(f'{len(all_seeds)-len(set(all_seeds))}')\n",
    "    raise IndexError('Not all seeds are unique !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 columns names found from generative function with A \n",
      "30 columns names found in df ending with A\n",
      "All rating items columns found, we're good to go.\n",
      "30 columns names found from generative function with B \n",
      "30 columns names found in df ending with B\n",
      "All rating items columns found, we're good to go.\n",
      "30 columns names found from generative function with C \n",
      "30 columns names found in df ending with C\n",
      "All rating items columns found, we're good to go.\n",
      "30 columns names found from generative function with D \n",
      "30 columns names found in df ending with D\n",
      "All rating items columns found, we're good to go.\n",
      "30 columns names found from generative function with E \n",
      "30 columns names found in df ending with E\n",
      "All rating items columns found, we're good to go.\n",
      "30 columns names found from generative function with F \n",
      "30 columns names found in df ending with F\n",
      "All rating items columns found, we're good to go.\n"
     ]
    }
   ],
   "source": [
    "#check if combine_questions_codes finds all rating item columns\n",
    "for stimulus_letter in conditions:\n",
    "    list_1 = combine_questions_codes(items, stimulus_letter, comb_position='after_item', prefix='', suffix='')\n",
    "    list_2 = []\n",
    "    for col in list(df):\n",
    "        if col[-1] == stimulus_letter and col[-2]!=\"M\" and col[-2]!=\"F\": #for TaskMC and F for TaskFE\n",
    "            list_2.append(col)\n",
    "\n",
    "    print(f'{len(list_1)} columns names found from generative function with {stimulus_letter}',\n",
    "        f'\\n{len(list_2)} columns names found in df ending with {stimulus_letter}')\n",
    "    if len(list_1) > len(list_2):\n",
    "        print(f'{[value for value in list_1 if value not in list_2]} missing from generative function')\n",
    "    elif len(list_2) > len(list_1):\n",
    "        print(f'{[value for value in list_2 if value not in list_1]} missing from the dataframe columns')\n",
    "    else:\n",
    "        print(\"All rating items columns found, we're good to go.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stimulus_df(original_df, stimulus_str, filtering_col, filtering_value):\n",
    "    print(f'=========\\nCreating df(s) for stimulus \"{stimulus_str}\"')\n",
    "    #filter rows with answers for this stimulus\n",
    "    stimulus_df = original_df.query(f'{filtering_col} == {filtering_value}')\n",
    "    \n",
    "    #drop times\n",
    "    stimulus_df = drop_times(stimulus_df)\n",
    "\n",
    "    #drop full NaNs\n",
    "    stimulus_df = drop_full_NaN_cols(stimulus_df)\n",
    "    \n",
    "    #create lists of answers codes\n",
    "    itemsAnswers = combine_questions_codes(items, stimulus_str, comb_position='after_item', prefix='', suffix='')\n",
    "\n",
    "    #create lists of reading task questions codes\n",
    "    taskAnswers = [code for code in list(stimulus_df.columns) if stimulus_str+'Task' in code]\n",
    "\n",
    "    #create answer df\n",
    "    colsAnswers = ['seed']+demographics+taskAnswers+itemsAnswers #cols to keep\n",
    "    #colsAnswers = itemsAnswers #cols to keep\n",
    "    dfAnswers = stimulus_df.filter(colsAnswers, axis=1) #filter df with cols to keep\n",
    "    dfAnswers = dfAnswers.replace(ratings) # Replace ratings by scores (or empty string for \"I don't know\")\n",
    "    dfAnswers = dfAnswers.mask(dfAnswers == '') #this replaces empty strings with NaNs (\"other\" selection)\n",
    "    #center scores on 0\n",
    "    for col_found in itemsAnswers:\n",
    "        dfAnswers[col_found] = dfAnswers[col_found].sub(4, fill_value=None, axis=0)\n",
    "\n",
    "    #if there are others, we create a df too\n",
    "    itemsOthers = [item+'_other_' for item in itemsAnswers if item+'_other_' in list(stimulus_df.columns)]\n",
    "    print(f'{len(itemsOthers)} other answers found') #since we alread removed full NaN columns\n",
    "    if len(itemsOthers)>0:\n",
    "        print(itemsOthers)\n",
    "        colsOthers = ['seed']+demographics+itemsOthers\n",
    "        dfOthers = stimulus_df.filter(colsOthers, axis=1)\n",
    "        \n",
    "        #extract comments only\n",
    "        dfOthers_comments = dfOthers.dropna(subset=itemsOthers, how='all')\n",
    "        if len(dfOthers_comments) > 0:\n",
    "            dfOthers_comments = dfOthers_comments.T\n",
    "        \n",
    "        dfOthers = dfOthers.T\n",
    "        \n",
    "    else:\n",
    "        dfOthers = False\n",
    "        dfOthers_comments = False\n",
    "        \n",
    "    return dfAnswers, dfOthers, dfOthers_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========\n",
      "Creating df(s) for stimulus \"A\"\n",
      "Dropped 207 Question time columns\n",
      "Dropped 250 fully empty columns\n",
      "['BTaskRV', 'BTaskFCT', 'BTaskTopic', 'answerB', 'answerB_other_', 'attentionCheckB', 'clearDataB', 'clearRepresentB', 'crowdB', 'complexB', 'complexB_other_', 'confidB', 'confusB', 'messiB', 'messiB_other_', 'deciphB', 'deciphB_other_', 'distinguishB', 'distractB', 'distractB_other_', 'effectB', 'findB', 'identifiB', 'informB', 'lostB', 'lostB_other_', 'meanElemB', 'meanOveralB', 'obviousB', 'organizB', 'representB', 'readB', 'readablB', 'readablB_other_', 'seeB', 'seeB_other_', 'simplB', 'understandEasiB', 'understandQuickB', 'valuB', 'visiblB', 'CTaskFE', 'CTaskMC', 'CTaskTopic', 'CTopicError', 'CTaskTopicTryAgain', 'answerC', 'attentionCheckC', 'clearDataC', 'clearRepresentC', 'clearRepresentC_other_', 'crowdC', 'complexC', 'confidC', 'confusC', 'confusC_other_', 'messiC', 'deciphC', 'distinguishC', 'distinguishC_other_', 'distractC', 'distractC_other_', 'effectC', 'findC', 'identifiC', 'informC', 'informC_other_', 'lostC', 'meanElemC', 'meanOveralC', 'obviousC', 'obviousC_other_', 'organizC', 'representC', 'representC_other_', 'readC', 'readC_other_', 'readablC', 'seeC', 'seeC_other_', 'simplC', 'understandEasiC', 'understandQuickC', 'understandQuickC_other_', 'valuC', 'valuC_other_', 'visiblC', 'visiblC_other_', 'DTaskFE', 'DTaskMC', 'DTaskTopic', 'DTopicError', 'DTaskTopicTryAgain', 'answerD', 'attentionCheckD', 'clearDataD', 'clearRepresentD', 'crowdD', 'crowdD_other_', 'complexD', 'complexD_other_', 'confidD', 'confidD_other_', 'confusD', 'confusD_other_', 'messiD', 'deciphD', 'deciphD_other_', 'distinguishD', 'distinguishD_other_', 'distractD', 'effectD', 'effectD_other_', 'findD', 'identifiD', 'identifiD_other_', 'informD', 'lostD', 'meanElemD', 'meanElemD_other_', 'meanOveralD', 'meanOveralD_other_', 'obviousD', 'organizD', 'organizD_other_', 'representD', 'representD_other_', 'readD', 'readD_other_', 'readablD', 'readablD_other_', 'seeD', 'simplD', 'simplD_other_', 'understandEasiD', 'understandEasiD_other_', 'understandQuickD', 'understandQuickD_other_', 'valuD', 'valuD_other_', 'visiblD', 'visiblD_other_', 'ETaskRV', 'ETaskCl', 'ETaskTopic', 'answerE', 'answerE_other_', 'attentionCheckE', 'clearDataE', 'clearRepresentE', 'crowdE', 'complexE', 'confidE', 'confusE', 'messiE', 'deciphE', 'distinguishE', 'distractE', 'effectE', 'findE', 'identifiE', 'informE', 'lostE', 'meanElemE', 'meanElemE_other_', 'meanOveralE', 'obviousE', 'obviousE_other_', 'organizE', 'representE', 'readE', 'readablE', 'seeE', 'simplE', 'understandEasiE', 'understandQuickE', 'valuE', 'valuE_other_', 'visiblE', 'FTaskRV', 'FTaskFCT', 'FTaskTopic', 'answerF', 'attentionCheckF', 'clearDataF', 'clearRepresentF', 'crowdF', 'complexF', 'confidF', 'confusF', 'messiF', 'deciphF', 'distinguishF', 'distractF', 'effectF', 'findF', 'identifiF', 'informF', 'lostF', 'meanElemF', 'meanOveralF', 'obviousF', 'organizF', 'organizF_other_', 'representF', 'readF', 'readablF', 'seeF', 'simplF', 'understandEasiF', 'understandQuickF', 'valuF', 'visiblF', 'FTopicError', 'FTaskTopicTryAgain', 'seeF_other_', 'clearDataE_other_', 'clearRepresentE_other_', 'complexE_other_', 'confidE_other_', 'distinguishE_other_', 'distractE_other_', 'effectE_other_', 'informE_other_', 'lostE_other_', 'meanOveralE_other_', 'organizE_other_', 'representE_other_', 'readE_other_', 'readablE_other_', 'seeE_other_', 'simplE_other_', 'understandEasiE_other_', 'understandQuickE_other_', 'visiblE_other_', 'answerF_other_', 'clearDataF_other_', 'complexF_other_', 'confidF_other_', 'confusF_other_', 'messiF_other_', 'deciphF_other_', 'distinguishF_other_', 'findF_other_', 'lostF_other_', 'meanOveralF_other_', 'readF_other_', 'simplF_other_', 'understandQuickF_other_', 'visiblF_other_']\n",
      "12 other answers found\n",
      "['answerA_other_', 'clearDataA_other_', 'clearRepresentA_other_', 'distinguishA_other_', 'effectA_other_', 'findA_other_', 'identifiA_other_', 'organizA_other_', 'readA_other_', 'readablA_other_', 'simplA_other_', 'valuA_other_']\n",
      "=========\n",
      "Creating df(s) for stimulus \"B\"\n",
      "Dropped 207 Question time columns\n",
      "Dropped 254 fully empty columns\n",
      "['ATaskRV', 'ATaskFE', 'ATaskTopic', 'answerA', 'answerA_other_', 'attentionCheckA', 'clearDataA', 'clearDataA_other_', 'clearRepresentA', 'clearRepresentA_other_', 'crowdA', 'complexA', 'confidA', 'confusA', 'messiA', 'deciphA', 'distinguishA', 'distinguishA_other_', 'distractA', 'effectA', 'effectA_other_', 'findA', 'findA_other_', 'identifiA', 'identifiA_other_', 'informA', 'lostA', 'meanElemA', 'meanOveralA', 'obviousA', 'organizA', 'organizA_other_', 'representA', 'readA', 'readA_other_', 'readablA', 'readablA_other_', 'seeA', 'simplA', 'simplA_other_', 'understandEasiA', 'understandQuickA', 'valuA', 'valuA_other_', 'visiblA', 'CTaskFE', 'CTaskMC', 'CTaskTopic', 'CTopicError', 'CTaskTopicTryAgain', 'answerC', 'attentionCheckC', 'clearDataC', 'clearRepresentC', 'clearRepresentC_other_', 'crowdC', 'complexC', 'confidC', 'confusC', 'confusC_other_', 'messiC', 'deciphC', 'distinguishC', 'distinguishC_other_', 'distractC', 'distractC_other_', 'effectC', 'findC', 'identifiC', 'informC', 'informC_other_', 'lostC', 'meanElemC', 'meanOveralC', 'obviousC', 'obviousC_other_', 'organizC', 'representC', 'representC_other_', 'readC', 'readC_other_', 'readablC', 'seeC', 'seeC_other_', 'simplC', 'understandEasiC', 'understandQuickC', 'understandQuickC_other_', 'valuC', 'valuC_other_', 'visiblC', 'visiblC_other_', 'DTaskFE', 'DTaskMC', 'DTaskTopic', 'DTopicError', 'DTaskTopicTryAgain', 'answerD', 'attentionCheckD', 'clearDataD', 'clearRepresentD', 'crowdD', 'crowdD_other_', 'complexD', 'complexD_other_', 'confidD', 'confidD_other_', 'confusD', 'confusD_other_', 'messiD', 'deciphD', 'deciphD_other_', 'distinguishD', 'distinguishD_other_', 'distractD', 'effectD', 'effectD_other_', 'findD', 'identifiD', 'identifiD_other_', 'informD', 'lostD', 'meanElemD', 'meanElemD_other_', 'meanOveralD', 'meanOveralD_other_', 'obviousD', 'organizD', 'organizD_other_', 'representD', 'representD_other_', 'readD', 'readD_other_', 'readablD', 'readablD_other_', 'seeD', 'simplD', 'simplD_other_', 'understandEasiD', 'understandEasiD_other_', 'understandQuickD', 'understandQuickD_other_', 'valuD', 'valuD_other_', 'visiblD', 'visiblD_other_', 'ETaskRV', 'ETaskCl', 'ETaskTopic', 'answerE', 'answerE_other_', 'attentionCheckE', 'clearDataE', 'clearRepresentE', 'crowdE', 'complexE', 'confidE', 'confusE', 'messiE', 'deciphE', 'distinguishE', 'distractE', 'effectE', 'findE', 'identifiE', 'informE', 'lostE', 'meanElemE', 'meanElemE_other_', 'meanOveralE', 'obviousE', 'obviousE_other_', 'organizE', 'representE', 'readE', 'readablE', 'seeE', 'simplE', 'understandEasiE', 'understandQuickE', 'valuE', 'valuE_other_', 'visiblE', 'FTaskRV', 'FTaskFCT', 'FTaskTopic', 'answerF', 'attentionCheckF', 'clearDataF', 'clearRepresentF', 'crowdF', 'complexF', 'confidF', 'confusF', 'messiF', 'deciphF', 'distinguishF', 'distractF', 'effectF', 'findF', 'identifiF', 'informF', 'lostF', 'meanElemF', 'meanOveralF', 'obviousF', 'organizF', 'organizF_other_', 'representF', 'readF', 'readablF', 'seeF', 'simplF', 'understandEasiF', 'understandQuickF', 'valuF', 'visiblF', 'FTopicError', 'FTaskTopicTryAgain', 'seeF_other_', 'clearDataE_other_', 'clearRepresentE_other_', 'complexE_other_', 'confidE_other_', 'distinguishE_other_', 'distractE_other_', 'effectE_other_', 'informE_other_', 'lostE_other_', 'meanOveralE_other_', 'organizE_other_', 'representE_other_', 'readE_other_', 'readablE_other_', 'seeE_other_', 'simplE_other_', 'understandEasiE_other_', 'understandQuickE_other_', 'visiblE_other_', 'answerF_other_', 'clearDataF_other_', 'complexF_other_', 'confidF_other_', 'confusF_other_', 'messiF_other_', 'deciphF_other_', 'distinguishF_other_', 'findF_other_', 'lostF_other_', 'meanOveralF_other_', 'readF_other_', 'simplF_other_', 'understandQuickF_other_', 'visiblF_other_']\n",
      "8 other answers found\n",
      "['answerB_other_', 'complexB_other_', 'deciphB_other_', 'distractB_other_', 'lostB_other_', 'messiB_other_', 'readablB_other_', 'seeB_other_']\n",
      "=========\n",
      "Creating df(s) for stimulus \"C\"\n",
      "Dropped 207 Question time columns\n",
      "Dropped 248 fully empty columns\n",
      "['ATaskRV', 'ATaskFE', 'ATaskTopic', 'answerA', 'answerA_other_', 'attentionCheckA', 'clearDataA', 'clearDataA_other_', 'clearRepresentA', 'clearRepresentA_other_', 'crowdA', 'complexA', 'confidA', 'confusA', 'messiA', 'deciphA', 'distinguishA', 'distinguishA_other_', 'distractA', 'effectA', 'effectA_other_', 'findA', 'findA_other_', 'identifiA', 'identifiA_other_', 'informA', 'lostA', 'meanElemA', 'meanOveralA', 'obviousA', 'organizA', 'organizA_other_', 'representA', 'readA', 'readA_other_', 'readablA', 'readablA_other_', 'seeA', 'simplA', 'simplA_other_', 'understandEasiA', 'understandQuickA', 'valuA', 'valuA_other_', 'visiblA', 'BTaskRV', 'BTaskFCT', 'BTaskTopic', 'answerB', 'answerB_other_', 'attentionCheckB', 'clearDataB', 'clearRepresentB', 'crowdB', 'complexB', 'complexB_other_', 'confidB', 'confusB', 'messiB', 'messiB_other_', 'deciphB', 'deciphB_other_', 'distinguishB', 'distractB', 'distractB_other_', 'effectB', 'findB', 'identifiB', 'informB', 'lostB', 'lostB_other_', 'meanElemB', 'meanOveralB', 'obviousB', 'organizB', 'representB', 'readB', 'readablB', 'readablB_other_', 'seeB', 'seeB_other_', 'simplB', 'understandEasiB', 'understandQuickB', 'valuB', 'visiblB', 'DTaskFE', 'DTaskMC', 'DTaskTopic', 'DTopicError', 'DTaskTopicTryAgain', 'answerD', 'attentionCheckD', 'clearDataD', 'clearRepresentD', 'crowdD', 'crowdD_other_', 'complexD', 'complexD_other_', 'confidD', 'confidD_other_', 'confusD', 'confusD_other_', 'messiD', 'deciphD', 'deciphD_other_', 'distinguishD', 'distinguishD_other_', 'distractD', 'effectD', 'effectD_other_', 'findD', 'identifiD', 'identifiD_other_', 'informD', 'lostD', 'meanElemD', 'meanElemD_other_', 'meanOveralD', 'meanOveralD_other_', 'obviousD', 'organizD', 'organizD_other_', 'representD', 'representD_other_', 'readD', 'readD_other_', 'readablD', 'readablD_other_', 'seeD', 'simplD', 'simplD_other_', 'understandEasiD', 'understandEasiD_other_', 'understandQuickD', 'understandQuickD_other_', 'valuD', 'valuD_other_', 'visiblD', 'visiblD_other_', 'ETaskRV', 'ETaskCl', 'ETaskTopic', 'answerE', 'answerE_other_', 'attentionCheckE', 'clearDataE', 'clearRepresentE', 'crowdE', 'complexE', 'confidE', 'confusE', 'messiE', 'deciphE', 'distinguishE', 'distractE', 'effectE', 'findE', 'identifiE', 'informE', 'lostE', 'meanElemE', 'meanElemE_other_', 'meanOveralE', 'obviousE', 'obviousE_other_', 'organizE', 'representE', 'readE', 'readablE', 'seeE', 'simplE', 'understandEasiE', 'understandQuickE', 'valuE', 'valuE_other_', 'visiblE', 'FTaskRV', 'FTaskFCT', 'FTaskTopic', 'answerF', 'attentionCheckF', 'clearDataF', 'clearRepresentF', 'crowdF', 'complexF', 'confidF', 'confusF', 'messiF', 'deciphF', 'distinguishF', 'distractF', 'effectF', 'findF', 'identifiF', 'informF', 'lostF', 'meanElemF', 'meanOveralF', 'obviousF', 'organizF', 'organizF_other_', 'representF', 'readF', 'readablF', 'seeF', 'simplF', 'understandEasiF', 'understandQuickF', 'valuF', 'visiblF', 'FTopicError', 'FTaskTopicTryAgain', 'seeF_other_', 'clearDataE_other_', 'clearRepresentE_other_', 'complexE_other_', 'confidE_other_', 'distinguishE_other_', 'distractE_other_', 'effectE_other_', 'informE_other_', 'lostE_other_', 'meanOveralE_other_', 'organizE_other_', 'representE_other_', 'readE_other_', 'readablE_other_', 'seeE_other_', 'simplE_other_', 'understandEasiE_other_', 'understandQuickE_other_', 'visiblE_other_', 'answerF_other_', 'clearDataF_other_', 'complexF_other_', 'confidF_other_', 'confusF_other_', 'messiF_other_', 'deciphF_other_', 'distinguishF_other_', 'findF_other_', 'lostF_other_', 'meanOveralF_other_', 'readF_other_', 'simplF_other_', 'understandQuickF_other_', 'visiblF_other_']\n",
      "12 other answers found\n",
      "['clearRepresentC_other_', 'confusC_other_', 'distinguishC_other_', 'distractC_other_', 'informC_other_', 'obviousC_other_', 'readC_other_', 'representC_other_', 'seeC_other_', 'understandQuickC_other_', 'valuC_other_', 'visiblC_other_']\n",
      "=========\n",
      "Creating df(s) for stimulus \"D\"\n",
      "Dropped 207 Question time columns\n",
      "Dropped 241 fully empty columns\n",
      "['ATaskRV', 'ATaskFE', 'ATaskTopic', 'answerA', 'answerA_other_', 'attentionCheckA', 'clearDataA', 'clearDataA_other_', 'clearRepresentA', 'clearRepresentA_other_', 'crowdA', 'complexA', 'confidA', 'confusA', 'messiA', 'deciphA', 'distinguishA', 'distinguishA_other_', 'distractA', 'effectA', 'effectA_other_', 'findA', 'findA_other_', 'identifiA', 'identifiA_other_', 'informA', 'lostA', 'meanElemA', 'meanOveralA', 'obviousA', 'organizA', 'organizA_other_', 'representA', 'readA', 'readA_other_', 'readablA', 'readablA_other_', 'seeA', 'simplA', 'simplA_other_', 'understandEasiA', 'understandQuickA', 'valuA', 'valuA_other_', 'visiblA', 'BTaskRV', 'BTaskFCT', 'BTaskTopic', 'answerB', 'answerB_other_', 'attentionCheckB', 'clearDataB', 'clearRepresentB', 'crowdB', 'complexB', 'complexB_other_', 'confidB', 'confusB', 'messiB', 'messiB_other_', 'deciphB', 'deciphB_other_', 'distinguishB', 'distractB', 'distractB_other_', 'effectB', 'findB', 'identifiB', 'informB', 'lostB', 'lostB_other_', 'meanElemB', 'meanOveralB', 'obviousB', 'organizB', 'representB', 'readB', 'readablB', 'readablB_other_', 'seeB', 'seeB_other_', 'simplB', 'understandEasiB', 'understandQuickB', 'valuB', 'visiblB', 'CTaskFE', 'CTaskMC', 'CTaskTopic', 'CTopicError', 'CTaskTopicTryAgain', 'answerC', 'attentionCheckC', 'clearDataC', 'clearRepresentC', 'clearRepresentC_other_', 'crowdC', 'complexC', 'confidC', 'confusC', 'confusC_other_', 'messiC', 'deciphC', 'distinguishC', 'distinguishC_other_', 'distractC', 'distractC_other_', 'effectC', 'findC', 'identifiC', 'informC', 'informC_other_', 'lostC', 'meanElemC', 'meanOveralC', 'obviousC', 'obviousC_other_', 'organizC', 'representC', 'representC_other_', 'readC', 'readC_other_', 'readablC', 'seeC', 'seeC_other_', 'simplC', 'understandEasiC', 'understandQuickC', 'understandQuickC_other_', 'valuC', 'valuC_other_', 'visiblC', 'visiblC_other_', 'ETaskRV', 'ETaskCl', 'ETaskTopic', 'answerE', 'answerE_other_', 'attentionCheckE', 'clearDataE', 'clearRepresentE', 'crowdE', 'complexE', 'confidE', 'confusE', 'messiE', 'deciphE', 'distinguishE', 'distractE', 'effectE', 'findE', 'identifiE', 'informE', 'lostE', 'meanElemE', 'meanElemE_other_', 'meanOveralE', 'obviousE', 'obviousE_other_', 'organizE', 'representE', 'readE', 'readablE', 'seeE', 'simplE', 'understandEasiE', 'understandQuickE', 'valuE', 'valuE_other_', 'visiblE', 'FTaskRV', 'FTaskFCT', 'FTaskTopic', 'answerF', 'attentionCheckF', 'clearDataF', 'clearRepresentF', 'crowdF', 'complexF', 'confidF', 'confusF', 'messiF', 'deciphF', 'distinguishF', 'distractF', 'effectF', 'findF', 'identifiF', 'informF', 'lostF', 'meanElemF', 'meanOveralF', 'obviousF', 'organizF', 'organizF_other_', 'representF', 'readF', 'readablF', 'seeF', 'simplF', 'understandEasiF', 'understandQuickF', 'valuF', 'visiblF', 'FTopicError', 'FTaskTopicTryAgain', 'seeF_other_', 'clearDataE_other_', 'clearRepresentE_other_', 'complexE_other_', 'confidE_other_', 'distinguishE_other_', 'distractE_other_', 'effectE_other_', 'informE_other_', 'lostE_other_', 'meanOveralE_other_', 'organizE_other_', 'representE_other_', 'readE_other_', 'readablE_other_', 'seeE_other_', 'simplE_other_', 'understandEasiE_other_', 'understandQuickE_other_', 'visiblE_other_', 'answerF_other_', 'clearDataF_other_', 'complexF_other_', 'confidF_other_', 'confusF_other_', 'messiF_other_', 'deciphF_other_', 'distinguishF_other_', 'findF_other_', 'lostF_other_', 'meanOveralF_other_', 'readF_other_', 'simplF_other_', 'understandQuickF_other_', 'visiblF_other_']\n",
      "19 other answers found\n",
      "['complexD_other_', 'confidD_other_', 'confusD_other_', 'crowdD_other_', 'deciphD_other_', 'distinguishD_other_', 'effectD_other_', 'identifiD_other_', 'meanElemD_other_', 'meanOveralD_other_', 'organizD_other_', 'readD_other_', 'readablD_other_', 'representD_other_', 'simplD_other_', 'understandEasiD_other_', 'understandQuickD_other_', 'valuD_other_', 'visiblD_other_']\n",
      "=========\n",
      "Creating df(s) for stimulus \"E\"\n",
      "Dropped 207 Question time columns\n",
      "Dropped 239 fully empty columns\n",
      "['ATaskRV', 'ATaskFE', 'ATaskTopic', 'answerA', 'answerA_other_', 'attentionCheckA', 'clearDataA', 'clearDataA_other_', 'clearRepresentA', 'clearRepresentA_other_', 'crowdA', 'complexA', 'confidA', 'confusA', 'messiA', 'deciphA', 'distinguishA', 'distinguishA_other_', 'distractA', 'effectA', 'effectA_other_', 'findA', 'findA_other_', 'identifiA', 'identifiA_other_', 'informA', 'lostA', 'meanElemA', 'meanOveralA', 'obviousA', 'organizA', 'organizA_other_', 'representA', 'readA', 'readA_other_', 'readablA', 'readablA_other_', 'seeA', 'simplA', 'simplA_other_', 'understandEasiA', 'understandQuickA', 'valuA', 'valuA_other_', 'visiblA', 'BTaskRV', 'BTaskFCT', 'BTaskTopic', 'answerB', 'answerB_other_', 'attentionCheckB', 'clearDataB', 'clearRepresentB', 'crowdB', 'complexB', 'complexB_other_', 'confidB', 'confusB', 'messiB', 'messiB_other_', 'deciphB', 'deciphB_other_', 'distinguishB', 'distractB', 'distractB_other_', 'effectB', 'findB', 'identifiB', 'informB', 'lostB', 'lostB_other_', 'meanElemB', 'meanOveralB', 'obviousB', 'organizB', 'representB', 'readB', 'readablB', 'readablB_other_', 'seeB', 'seeB_other_', 'simplB', 'understandEasiB', 'understandQuickB', 'valuB', 'visiblB', 'CTaskFE', 'CTaskMC', 'CTaskTopic', 'CTopicError', 'CTaskTopicTryAgain', 'answerC', 'attentionCheckC', 'clearDataC', 'clearRepresentC', 'clearRepresentC_other_', 'crowdC', 'complexC', 'confidC', 'confusC', 'confusC_other_', 'messiC', 'deciphC', 'distinguishC', 'distinguishC_other_', 'distractC', 'distractC_other_', 'effectC', 'findC', 'identifiC', 'informC', 'informC_other_', 'lostC', 'meanElemC', 'meanOveralC', 'obviousC', 'obviousC_other_', 'organizC', 'representC', 'representC_other_', 'readC', 'readC_other_', 'readablC', 'seeC', 'seeC_other_', 'simplC', 'understandEasiC', 'understandQuickC', 'understandQuickC_other_', 'valuC', 'valuC_other_', 'visiblC', 'visiblC_other_', 'DTaskFE', 'DTaskMC', 'DTaskTopic', 'DTopicError', 'DTaskTopicTryAgain', 'answerD', 'attentionCheckD', 'clearDataD', 'clearRepresentD', 'crowdD', 'crowdD_other_', 'complexD', 'complexD_other_', 'confidD', 'confidD_other_', 'confusD', 'confusD_other_', 'messiD', 'deciphD', 'deciphD_other_', 'distinguishD', 'distinguishD_other_', 'distractD', 'effectD', 'effectD_other_', 'findD', 'identifiD', 'identifiD_other_', 'informD', 'lostD', 'meanElemD', 'meanElemD_other_', 'meanOveralD', 'meanOveralD_other_', 'obviousD', 'organizD', 'organizD_other_', 'representD', 'representD_other_', 'readD', 'readD_other_', 'readablD', 'readablD_other_', 'seeD', 'simplD', 'simplD_other_', 'understandEasiD', 'understandEasiD_other_', 'understandQuickD', 'understandQuickD_other_', 'valuD', 'valuD_other_', 'visiblD', 'visiblD_other_', 'FTaskRV', 'FTaskFCT', 'FTaskTopic', 'answerF', 'attentionCheckF', 'clearDataF', 'clearRepresentF', 'crowdF', 'complexF', 'confidF', 'confusF', 'messiF', 'deciphF', 'distinguishF', 'distractF', 'effectF', 'findF', 'identifiF', 'informF', 'lostF', 'meanElemF', 'meanOveralF', 'obviousF', 'organizF', 'organizF_other_', 'representF', 'readF', 'readablF', 'seeF', 'simplF', 'understandEasiF', 'understandQuickF', 'valuF', 'visiblF', 'FTopicError', 'FTaskTopicTryAgain', 'seeF_other_', 'answerF_other_', 'clearDataF_other_', 'complexF_other_', 'confidF_other_', 'confusF_other_', 'messiF_other_', 'deciphF_other_', 'distinguishF_other_', 'findF_other_', 'lostF_other_', 'meanOveralF_other_', 'readF_other_', 'simplF_other_', 'understandQuickF_other_', 'visiblF_other_']\n",
      "23 other answers found\n",
      "['answerE_other_', 'clearDataE_other_', 'clearRepresentE_other_', 'complexE_other_', 'confidE_other_', 'distinguishE_other_', 'distractE_other_', 'effectE_other_', 'informE_other_', 'lostE_other_', 'meanElemE_other_', 'meanOveralE_other_', 'obviousE_other_', 'organizE_other_', 'readE_other_', 'readablE_other_', 'representE_other_', 'seeE_other_', 'simplE_other_', 'understandEasiE_other_', 'understandQuickE_other_', 'valuE_other_', 'visiblE_other_']\n",
      "=========\n",
      "Creating df(s) for stimulus \"F\"\n",
      "Dropped 207 Question time columns\n",
      "Dropped 244 fully empty columns\n",
      "['ATaskRV', 'ATaskFE', 'ATaskTopic', 'answerA', 'answerA_other_', 'attentionCheckA', 'clearDataA', 'clearDataA_other_', 'clearRepresentA', 'clearRepresentA_other_', 'crowdA', 'complexA', 'confidA', 'confusA', 'messiA', 'deciphA', 'distinguishA', 'distinguishA_other_', 'distractA', 'effectA', 'effectA_other_', 'findA', 'findA_other_', 'identifiA', 'identifiA_other_', 'informA', 'lostA', 'meanElemA', 'meanOveralA', 'obviousA', 'organizA', 'organizA_other_', 'representA', 'readA', 'readA_other_', 'readablA', 'readablA_other_', 'seeA', 'simplA', 'simplA_other_', 'understandEasiA', 'understandQuickA', 'valuA', 'valuA_other_', 'visiblA', 'BTaskRV', 'BTaskFCT', 'BTaskTopic', 'answerB', 'answerB_other_', 'attentionCheckB', 'clearDataB', 'clearRepresentB', 'crowdB', 'complexB', 'complexB_other_', 'confidB', 'confusB', 'messiB', 'messiB_other_', 'deciphB', 'deciphB_other_', 'distinguishB', 'distractB', 'distractB_other_', 'effectB', 'findB', 'identifiB', 'informB', 'lostB', 'lostB_other_', 'meanElemB', 'meanOveralB', 'obviousB', 'organizB', 'representB', 'readB', 'readablB', 'readablB_other_', 'seeB', 'seeB_other_', 'simplB', 'understandEasiB', 'understandQuickB', 'valuB', 'visiblB', 'CTaskFE', 'CTaskMC', 'CTaskTopic', 'CTopicError', 'CTaskTopicTryAgain', 'answerC', 'attentionCheckC', 'clearDataC', 'clearRepresentC', 'clearRepresentC_other_', 'crowdC', 'complexC', 'confidC', 'confusC', 'confusC_other_', 'messiC', 'deciphC', 'distinguishC', 'distinguishC_other_', 'distractC', 'distractC_other_', 'effectC', 'findC', 'identifiC', 'informC', 'informC_other_', 'lostC', 'meanElemC', 'meanOveralC', 'obviousC', 'obviousC_other_', 'organizC', 'representC', 'representC_other_', 'readC', 'readC_other_', 'readablC', 'seeC', 'seeC_other_', 'simplC', 'understandEasiC', 'understandQuickC', 'understandQuickC_other_', 'valuC', 'valuC_other_', 'visiblC', 'visiblC_other_', 'DTaskFE', 'DTaskMC', 'DTaskTopic', 'DTopicError', 'DTaskTopicTryAgain', 'answerD', 'attentionCheckD', 'clearDataD', 'clearRepresentD', 'crowdD', 'crowdD_other_', 'complexD', 'complexD_other_', 'confidD', 'confidD_other_', 'confusD', 'confusD_other_', 'messiD', 'deciphD', 'deciphD_other_', 'distinguishD', 'distinguishD_other_', 'distractD', 'effectD', 'effectD_other_', 'findD', 'identifiD', 'identifiD_other_', 'informD', 'lostD', 'meanElemD', 'meanElemD_other_', 'meanOveralD', 'meanOveralD_other_', 'obviousD', 'organizD', 'organizD_other_', 'representD', 'representD_other_', 'readD', 'readD_other_', 'readablD', 'readablD_other_', 'seeD', 'simplD', 'simplD_other_', 'understandEasiD', 'understandEasiD_other_', 'understandQuickD', 'understandQuickD_other_', 'valuD', 'valuD_other_', 'visiblD', 'visiblD_other_', 'ETaskRV', 'ETaskCl', 'ETaskTopic', 'answerE', 'answerE_other_', 'attentionCheckE', 'clearDataE', 'clearRepresentE', 'crowdE', 'complexE', 'confidE', 'confusE', 'messiE', 'deciphE', 'distinguishE', 'distractE', 'effectE', 'findE', 'identifiE', 'informE', 'lostE', 'meanElemE', 'meanElemE_other_', 'meanOveralE', 'obviousE', 'obviousE_other_', 'organizE', 'representE', 'readE', 'readablE', 'seeE', 'simplE', 'understandEasiE', 'understandQuickE', 'valuE', 'valuE_other_', 'visiblE', 'clearDataE_other_', 'clearRepresentE_other_', 'complexE_other_', 'confidE_other_', 'distinguishE_other_', 'distractE_other_', 'effectE_other_', 'informE_other_', 'lostE_other_', 'meanOveralE_other_', 'organizE_other_', 'representE_other_', 'readE_other_', 'readablE_other_', 'seeE_other_', 'simplE_other_', 'understandEasiE_other_', 'understandQuickE_other_', 'visiblE_other_', 'complexF_other_']\n",
      "16 other answers found\n",
      "['answerF_other_', 'clearDataF_other_', 'confidF_other_', 'confusF_other_', 'deciphF_other_', 'distinguishF_other_', 'findF_other_', 'lostF_other_', 'meanOveralF_other_', 'messiF_other_', 'organizF_other_', 'readF_other_', 'seeF_other_', 'simplF_other_', 'understandQuickF_other_', 'visiblF_other_']\n"
     ]
    }
   ],
   "source": [
    "#we run for each stimuli letter & corresponding rand number\n",
    "letter_to_number = {\n",
    "    'A':1,\n",
    "    'B':2,\n",
    "    'C':3,\n",
    "    'D':4,\n",
    "    'E':5,\n",
    "    'F':6\n",
    "}\n",
    "\n",
    "#we create a dict to hold our separate dfs\n",
    "initial_dfs = {}\n",
    "\n",
    "for condition in conditions:\n",
    "    cond_dfs = create_stimulus_df(df, condition, 'rand', letter_to_number[condition])\n",
    "    \n",
    "    #spread the results in the dict\n",
    "    initial_dfs.update({\n",
    "        condition: cond_dfs[0],\n",
    "        f'{condition}_others': cond_dfs[1],\n",
    "        f'{condition}_others_comments': cond_dfs[2]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export stimuli full answers tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported A as csv\n",
      "Exported A_others as csv in \"other\" dir\n",
      "Exported A_others_comments as csv in \"other\" dir\n",
      "Exported B as csv\n",
      "Exported B_others as csv in \"other\" dir\n",
      "Exported B_others_comments as csv in \"other\" dir\n",
      "Exported C as csv\n",
      "Exported C_others as csv in \"other\" dir\n",
      "Exported C_others_comments as csv in \"other\" dir\n",
      "Exported D as csv\n",
      "Exported D_others as csv in \"other\" dir\n",
      "Exported D_others_comments as csv in \"other\" dir\n",
      "Exported E as csv\n",
      "Exported E_others as csv in \"other\" dir\n",
      "Exported E_others_comments as csv in \"other\" dir\n",
      "Exported F as csv\n",
      "Exported F_others as csv in \"other\" dir\n",
      "Exported F_others_comments as csv in \"other\" dir\n"
     ]
    }
   ],
   "source": [
    "#csv export\n",
    "for k, v in initial_dfs.items():\n",
    "    if \"others\" not in k and isinstance(v, pd.DataFrame):\n",
    "        v.to_csv(f'{outdir}/initial-results-{k}.csv')\n",
    "        print(f'Exported {k} as csv')\n",
    "    elif \"comments\" not in k and isinstance(v, pd.DataFrame):\n",
    "        v.to_csv(f'{outdir}/others/intial-results-{k}.csv', header=False)\n",
    "        print(f'Exported {k} as csv in \"other\" dir')\n",
    "    elif isinstance(v, pd.DataFrame):\n",
    "        v.to_csv(f'{outdir}/others/intial-results-{k}.csv', header=False)\n",
    "        print(f'Exported {k} as csv in \"other\" dir')\n",
    "    elif v == False:\n",
    "        print(f'{k} has no \"I don\\'t know\" answer to export')\n",
    "    else:\n",
    "        print(f'{k}: {type(v)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participants exclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions we will use here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6 participants exluded for condition B (294 participants in total).\n",
      "= 97.96% valid answers\n"
     ]
    }
   ],
   "source": [
    "def count_rows_NaNs(my_df):\n",
    "    #create a col to store the value in the df\n",
    "    my_df['NaN_counts']=''\n",
    "    for i in my_df.index:\n",
    "        nan_count = my_df.loc[[i]].isna().sum().sum()\n",
    "        my_df.at[i, 'NaN_counts'] = nan_count\n",
    "    return my_df\n",
    "\n",
    "def exclude_participants(this_condition, correct_answers_dict, df_dict, NA_threshold = 12, topic_attempts_threshold = 2, multiple_exclusion_processing = 'join'):\n",
    "    this_df = df_dict[this_condition]\n",
    "    initial_len = len(this_df)\n",
    "\n",
    "    # incorrect answers on first attempt at topic\n",
    "    topic_code = this_condition+'TaskTopic'\n",
    "    this_correct_answer = correct_answers_dict[this_condition][topic_code]\n",
    "    dfTopicExcluded = this_df[this_df[topic_code] != this_correct_answer]\n",
    "    dfTopicExcluded.insert(0,\"exclusion\",\"Failed topic at first attempt\")\n",
    "\n",
    "    #incorrect rating in attentionCheck item\n",
    "    dfCalibrationExcluded = this_df[this_df[f'attentionCheck{this_condition}'] != 1]\n",
    "    dfCalibrationExcluded.insert(0,\"exclusion\",\"Failed attention check\")\n",
    "    \n",
    "    # more than 40% (more than 11 out of 29) \"I don't know\" selections\n",
    "    this_df = count_rows_NaNs(this_df)\n",
    "    dfNaNExcluded = this_df[this_df['NaN_counts'] >= NA_threshold]\n",
    "    dfNaNExcluded.insert(0,\"exclusion\",\"Over 40 percent of DK/NA answers\")\n",
    "\n",
    "    this_exclusion_dfs = {}\n",
    "\n",
    "    this_exclusion_dfs.update({\n",
    "        'wrong_calibration':dfCalibrationExcluded,\n",
    "        'too_many_NA':dfNaNExcluded,\n",
    "    })\n",
    "\n",
    "    #move this up if you want this reason to have priority over the others for participants with multiple exclusion causes\n",
    "    if topic_attempts_threshold == 1:\n",
    "        this_exclusion_dfs.update({\n",
    "            'wrong_topic':dfTopicExcluded\n",
    "        })\n",
    "\n",
    "\n",
    "    #all excluded participants\n",
    "    merged_df = pd.concat(\n",
    "        [df for df in this_exclusion_dfs.values()],\n",
    "        axis=0,\n",
    "        join=\"outer\",\n",
    "        ignore_index=False,\n",
    "        keys=None,\n",
    "        levels=None,\n",
    "        names=None,\n",
    "        verify_integrity=False,\n",
    "        copy=True,\n",
    "    )\n",
    "\n",
    "    #drop exact duplicates (is it still needed??)\n",
    "    merged_df = merged_df.drop_duplicates(subset=None, keep='first')\n",
    "\n",
    "    # some people have multiple reasons for exclusion\n",
    "    if multiple_exclusion_processing == 'join': ## this option joins the string contents in \"exclusion\" column\n",
    "        aggregate_parameters = {}\n",
    "        for col in list(merged_df):\n",
    "            if col == 'exclusion':\n",
    "                aggregate_parameters.update({\n",
    "                    col: ', '.join,\n",
    "                })\n",
    "            else:\n",
    "                aggregate_parameters.update({\n",
    "                    col: 'first',\n",
    "                })\n",
    "    \n",
    "        merged_df = merged_df.groupby('seed').agg(aggregate_parameters)\n",
    "    \n",
    "    \n",
    "    elif multiple_exclusion_processing == 'first': ## this options just keeps the first for all (so in order of dict iteration = order of insertion, garanteed in python 3.11)\n",
    "        merged_df = merged_df.groupby('seed').agg('first')\n",
    "\n",
    "    this_exclusion_dfs.update({\n",
    "        'all':merged_df,\n",
    "    })\n",
    "\n",
    "    excluded_len = len(merged_df)\n",
    "    print(f'\\n{excluded_len} participants exluded for condition {this_condition} ({initial_len} participants in total).')\n",
    "    print(f'= {round((1-(excluded_len/initial_len))*100,2)}% valid answers')\n",
    "\n",
    "    return this_exclusion_dfs\n",
    "\n",
    "\n",
    "#test of option 1 in multiple reason processing: join strings\n",
    "this = exclude_participants('B', correct_answers, initial_dfs, NA_threshold=12, topic_attempts_threshold=1, multiple_exclusion_processing = 'join')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We set aside excluded participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3 participants exluded for condition A (291 participants in total).\n",
      "= 98.97% valid answers\n",
      "\n",
      "6 participants exluded for condition B (294 participants in total).\n",
      "= 97.96% valid answers\n",
      "\n",
      "6 participants exluded for condition C (293 participants in total).\n",
      "= 97.95% valid answers\n",
      "\n",
      "6 participants exluded for condition D (315 participants in total).\n",
      "= 98.1% valid answers\n",
      "\n",
      "6 participants exluded for condition E (299 participants in total).\n",
      "= 97.99% valid answers\n",
      "\n",
      "6 participants exluded for condition F (299 participants in total).\n",
      "= 97.99% valid answers\n"
     ]
    }
   ],
   "source": [
    "exclusion_dfs = {}\n",
    "\n",
    "for condition in conditions:\n",
    "    exclusion_dfs[condition] = exclude_participants(condition,\n",
    "                                                    correct_answers,\n",
    "                                                    initial_dfs,\n",
    "                                                    NA_threshold=12, # 40% of 29 items = 11.6 so 12 and more\n",
    "                                                    topic_attempts_threshold=1, # 1 means we exclude based on wrong answer at first attempt. 2 means we keep all answers because people who were wrong twice got redirected out of the survey immediately\n",
    "                                                    multiple_exclusion_processing = 'join')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported all exclusions for A\n",
      "Exported all exclusions for B\n",
      "Exported all exclusions for C\n",
      "Exported all exclusions for D\n",
      "Exported all exclusions for E\n",
      "Exported all exclusions for F\n"
     ]
    }
   ],
   "source": [
    "#csv export\n",
    "for condition in conditions:\n",
    "    for k, v in exclusion_dfs[condition].items():\n",
    "        if k == 'all' and isinstance(v, pd.DataFrame):\n",
    "            v.to_csv(f'{outdir}/excluded-{condition}.csv')\n",
    "            print(f\"Exported {k} exclusions for {condition}\")\n",
    "        elif k == 'all': \n",
    "            print(f'{k}: {type(v)}') #in case something weird happened\n",
    "        # else:\n",
    "        #     print(f\"Didn't export {k} for {condition}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping and counting exclusions to report them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_exclusions_dfs = []\n",
    "\n",
    "for condition in conditions:\n",
    "    for k, v in exclusion_dfs[condition].items():\n",
    "        if k == 'all' and isinstance(v, pd.DataFrame):\n",
    "            v.insert(1,\"Condition\",condition)\n",
    "            all_exclusions_dfs += [v]\n",
    "\n",
    "exclusions_df = pd.concat(all_exclusions_dfs, axis=0)\n",
    "\n",
    "#export grouped by reason (all together)\n",
    "all_exclusions_df = exclusions_df.groupby(['exclusion']).size().reset_index(name=\"nb excluded\")\n",
    "all_exclusions_df.to_csv(f'{outdir}/excluded-all-counts.csv', index=False)\n",
    "\n",
    "#export groupes by reason and by stimulus\n",
    "exclusions_by_stimulus_df = exclusions_df.groupby(['Condition', 'exclusion']).size().reset_index(name=\"nb excluded\")\n",
    "exclusions_by_stimulus_df.to_csv(f'{outdir}/excluded-by_stimulus-counts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valid participations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions we will use here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we remove participants excluded in the previous step\n",
    "def remove_excluded_p(my_condition, my_initial_dfs, my_exclusion_dfs):\n",
    "    #get list of participants\n",
    "    all_p_list = my_initial_dfs[my_condition].index\n",
    "    #get list of excluded participants\n",
    "    excluded_p_list = my_exclusion_dfs[my_condition]['all'].index\n",
    "    #filter initial df\n",
    "    clean_df = my_initial_dfs[my_condition].filter(items = [p for p in all_p_list if p not in excluded_p_list], axis=0)\n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create \"clean\" dfs with valid answers only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 for A\n",
      "288 for B\n",
      "287 for C\n",
      "309 for D\n",
      "293 for E\n",
      "293 for F\n"
     ]
    }
   ],
   "source": [
    "# we create a dictionary to hold clean dfs\n",
    "clean_dfs = {\n",
    "    'A':{},\n",
    "    'B':{},\n",
    "    'C':{},\n",
    "    'D':{},\n",
    "    'E':{},\n",
    "    'F':{}\n",
    "}\n",
    "for condition in conditions:\n",
    "    clean_dfs[condition]['valid_answers'] = remove_excluded_p(condition, initial_dfs, exclusion_dfs)\n",
    "\n",
    "for k, v in clean_dfs.items():\n",
    "    if k in conditions: #avoid error for runs with partial conditions only\n",
    "        print(f\"{len(v['valid_answers'])} for {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['wrong_calibration', 'too_many_NA', 'wrong_topic', 'all'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclusion_dfs['A'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>Total</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>valid_participations</th>\n",
       "      <td>288</td>\n",
       "      <td>288</td>\n",
       "      <td>287</td>\n",
       "      <td>309</td>\n",
       "      <td>293</td>\n",
       "      <td>293</td>\n",
       "      <td>1758</td>\n",
       "      <td>293.0</td>\n",
       "      <td>8.270429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excluded_participations</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.224745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           A    B    C    D    E    F  Total   Mean       Std\n",
       "valid_participations     288  288  287  309  293  293   1758  293.0  8.270429\n",
       "excluded_participations    3    6    6    6    6    6     33    5.5  1.224745"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reporting table\n",
    "\n",
    "report = {\n",
    "    'valid_participations' : [len(clean_dfs[c]['valid_answers']) for c in conditions],\n",
    "    'excluded_participations' : [len(exclusion_dfs[c]['all']) for c in conditions],\n",
    "}\n",
    "\n",
    "report_df = pd.DataFrame.from_dict(report, orient='index', columns=conditions)\n",
    "report_df['Total'] = report_df.sum(axis=1)\n",
    "report_df['Mean'] = report_df[conditions].mean(axis=1)\n",
    "report_df['Std'] = report_df[conditions].std(axis=1)\n",
    "report_df.to_csv(f'{outdir}/all-participations-counts.csv')\n",
    "report_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract valid items ratings for EFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 items for A\n",
      "29 items for B\n",
      "29 items for C\n",
      "29 items for D\n",
      "29 items for E\n",
      "29 items for F\n"
     ]
    }
   ],
   "source": [
    "#for EFA we will only use items ratings\n",
    "ratingItems = copy.deepcopy(items)\n",
    "ratingItems.remove('attentionCheck')\n",
    "for condition in conditions:\n",
    "    itemsAnswers = combine_questions_codes(ratingItems, condition, comb_position='after_item', prefix='', suffix='')\n",
    "    dfRatings = clean_dfs[condition]['valid_answers'].filter(itemsAnswers, axis=1)\n",
    "    # we remove the letter from the item's name:\n",
    "    for col in list(dfRatings):\n",
    "        dfRatings.rename(columns={col:col[:-1]}, inplace=True)\n",
    "    clean_dfs[condition]['ratings'] = dfRatings\n",
    "    print(f\"{len(dfRatings.columns)} items for {condition}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['valid_answers', 'ratings'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_dfs['A'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_demographics(this_df, col_key, my_demographics):\n",
    "    #check we have demographics\n",
    "    missing_d = [d for d in my_demographics if d not in this_df.columns]\n",
    "    if len(missing_d) > 0:\n",
    "        print(f'Demographics are missing: {missing_d} in {col_key}')\n",
    "        updated_demographics = [demo for demo in my_demographics if demo not in missing_d]\n",
    "        my_demographics = updated_demographics\n",
    "        # return ''\n",
    "    \n",
    "    this_data = {\n",
    "        'Number of participants':len(this_df)\n",
    "    }\n",
    "    \n",
    "    for d in my_demographics:\n",
    "        if d == 'Age':\n",
    "            this_df[d] = this_df[d].astype('Int64')\n",
    "            this_data.update({\n",
    "                f'{d} - average':this_df[d].mean().astype(float).round(3),\n",
    "                f'{d} - std':int(this_df[d].std()),\n",
    "                f'{d} - min':int(this_df[d].min()),\n",
    "                f'{d} - max':int(this_df[d].max()),\n",
    "            })\n",
    "        else:\n",
    "            grouped_df = this_df.groupby([d])[d].count()\n",
    "            for value in grouped_df.index:\n",
    "                this_data.update({\n",
    "                    f'{d} - {value}':grouped_df.loc[value]\n",
    "                })\n",
    "\n",
    "    out_df = pd.DataFrame.from_dict(this_data, orient='index', columns=[col_key])\n",
    "    out_df[col_key] = out_df[col_key].astype(object)\n",
    "    return out_df\n",
    "\n",
    "all_demographics = []\n",
    "for condition in conditions:\n",
    "    this_df = clean_dfs[condition]['valid_answers']\n",
    "    this_demographics = retrieve_demographics(this_df, condition, demographics)\n",
    "    clean_dfs[condition].update({\n",
    "        'demographics': this_demographics\n",
    "    })\n",
    "    all_demographics.append(this_demographics)\n",
    "\n",
    "all_demo_df = pd.concat(all_demographics, axis=1)\n",
    "all_demo_df['Mean'] = all_demo_df[conditions].mean(axis=1).astype(float).round(3)\n",
    "all_demo_df['Std'] = all_demo_df[conditions].std(axis=1).astype(float).round(3)\n",
    "all_demo_df['Sum'] = all_demo_df[conditions].sum(axis=1)\n",
    "all_demo_df['Freq'] = all_demo_df[conditions].sum(axis=1)/all_demo_df.at['Number of participants','Sum']\n",
    "# all_demo_df.round(3).to_csv(f'{outdir}/all_demographics.csv', float_format=\"%.3f\")\n",
    "\n",
    "#we get global mean and std from the original df\n",
    "all_demo_df.at['Age - average', 'Sum'] = df['Age'].mean()\n",
    "all_demo_df.at['Age - std', 'Sum'] = df['Age'].std()\n",
    "\n",
    "all_demo_df.to_csv(f'{outdir}/all_demographics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we replace letters with numbers for R script\n",
    "numbers = {\n",
    "    'A':'1',\n",
    "    'B':'2',\n",
    "    'C':'3',\n",
    "    'D':'4',\n",
    "    'E':'5',\n",
    "    'F':'6'\n",
    "}\n",
    "\n",
    "#csv export\n",
    "ratings_df_for_agg = []\n",
    "for condition in conditions:\n",
    "    for k, v in clean_dfs[condition].items():\n",
    "        if isinstance(v, pd.DataFrame):\n",
    "            match k:\n",
    "                case 'ratings':\n",
    "                    v.to_csv(f'{outdir}/data/{k}-{numbers[condition]}.csv')\n",
    "                    v.insert(0,'stimulus',f'{condition}')\n",
    "                    ratings_df_for_agg.append(v)\n",
    "                case 'valid_answers' | 'demographics':\n",
    "                    v.to_csv(f'{outdir}/{k}-{condition}.csv')\n",
    "                case _:\n",
    "                      print(f'Something weird happened in {k} with df {v}')\n",
    "        else: \n",
    "            print(f'{k}: {type(v)}') #in case something weird happened\n",
    "\n",
    "#concat all ratings df with the stimulus col\n",
    "pd.concat(ratings_df_for_agg).to_csv((f'{outdir}/data/ratings-stimulus.csv'))\n",
    "#concat all ratings df without the stimulus col\n",
    "pd.concat([df.drop('stimulus', axis=1) for df in ratings_df_for_agg]).to_csv((f'{outdir}/data/ratings-7.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
